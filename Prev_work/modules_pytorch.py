# -*- coding: utf-8 -*-
"""Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gZNrmAhB3iV1BN5k6YXgfd1veHW9N-qD

### This notebook defines and test the indiviual modules, and the models
**To do**


*   Need MLP for Emitter : i.e $p(x_t|z_t)$ should output a mean and a log_cov vectors for a Guassian, also need a seperate binomials for binary variables. 
*   $p(z_t|z_{t-1},u_{t-1})$ Gated transitions 
* Guide for the posterior apporximation
* Also need (two?) RNNs to define $G_{\alpha}(z_{t-1},u_{t-1})), F_{\beta}(z_{t-1} ,u_{t-1})$


Let's start with using Pyro for inference

For the moment ignoring the binary parts
"""

import torch
import numpy as np
import pandas as pd
import datetime as dt
import random
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset,DataLoader
import torch.nn.functional as F
import torch.nn.init as weight_init
import os
import glob
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence,pad_packed_sequence

device='cuda' if torch.cuda.is_available() else 'cpu'

class Emitter(nn.Module):
  """
  A MLP representing  p(x_t|z_t)
  """
  def __init__(self,z_dim,continous_dim,binary_dim=None,hidden_size=128,hidden_layers=2):
    
    super(Emitter,self).__init__()
    
    self.bn=nn.BatchNorm1d(z_dim)
    self.x_dim=continous_dim
    self.linear=nn.Linear(z_dim,hidden_size)
    self.hidden_layers=nn.ModuleList([nn.Sequential(nn.Linear(hidden_size,hidden_size),nn.ELU()) for i in range(hidden_layers)])
    self.binary_dim=binary_dim
    self.linear_mu=nn.Linear(hidden_size,continous_dim)
    self.linear_sigma=nn.Linear(hidden_size,continous_dim)
    
    if binary_dim:
      self.linear_binary=nn.Linear(hidden_size,binary_dim)

  def forward(self,z):
    """
    z has dimensions #batch_size*hidden_size (B*Z_d)
    """
    z=self.linear(self.bn(z))  #B*Z_d
    
    for layer in self.hidden_layers:
          z=layer(z)    #B*H
    
    mu=self.linear_mu(z)  #B*X_d
    logvar=(self.linear_sigma(z)) #B*X_d
    # sampling z by re-parameterization 
    epsilon = torch.randn((z.size(0),self.x_dim)).to(device)  #Batch_Size*X_d
    
    if self.binary_dim:
      x_t = mu + epsilon * torch.exp(0.5 * logvar) 
      binary=F.sigmoid(self.linear_binary(z))
      return mu, logvar,binary

    
    x_t = mu + epsilon * torch.exp(0.5 * logvar) 
    return x_t,mu, logvar

"""### Gated Transitions"""

class Gated_Transition(nn.Module):
    """
    Parameterizes the gaussian latent transition probability p(z_t | z_{t-1},u_{t-1})
    using a MLP, where z is latent and u is the action
    Modified from https://pyro.ai/examples/dmm.html

    """
    def __init__(self, z_dim,u_dim, hidden_dim,n_layers=1):
      super(Gated_Transition,self).__init__()
      
      input_dim=z_dim+u_dim
      self.bn=nn.BatchNorm1d(input_dim)
      self.input_to_h=nn.Linear(z_dim+u_dim, hidden_dim)
      self.hidden_layers=nn.ModuleList([nn.Sequential(nn.Linear(hidden_dim,hidden_dim),nn.ELU()) for i in range(n_layers)])
      self.hidden_to_z=nn.Linear(hidden_dim,z_dim)
      
      self.proposed_mean_to_h=nn.Linear(z_dim+u_dim, hidden_dim)
      self.proposed_h_to_z=nn.Linear(hidden_dim,z_dim)

      self.input_to_mean=nn.Linear(z_dim+u_dim,z_dim)
      self.input_to_scale=nn.Linear(z_dim+u_dim,z_dim)
      self.input_to_hid_scale=nn.Linear(z_dim+u_dim,hidden_dim)
      self.hidden_to_scale=nn.Linear(hidden_dim,z_dim)

      self.input_to_mean.bias.data = torch.zeros(z_dim)


    def forward(self,z,u):
  
      """
        Given the latent z_{t-1} and the action u_{t-1}
        
        we return the mean and scale vectors that parameterize the
        (diagonal) gaussian distribution p(z_t | z_{t-1},u_{t-1})
        
        z_t_1 has shape B*D 
        u_t_1 has shape B*T
      """
      # concatenate the latent z and actions along the frequency dimension
      input_=torch.cat([z,u],dim=1)  #B*(D+T) let's call D+T=F
      input_=self.bn(input_)

      #calculate the gate
      # Add hidden layers if necessary
      
      _gate=F.elu(self.input_to_h(input_))
      gate=F.sigmoid(self.hidden_to_z(_gate))
      

      # compute the 'proposed mean'
    
      _proposed_mean = F.elu(self.proposed_mean_to_h(input_))  #B*U_D-->B*H_D   
      for layer in self.hidden_layers:
          _proposed_mean=layer(_proposed_mean)   #B*H_D
    
      proposed_mean = self.proposed_h_to_z(_proposed_mean)  #B*H_D--->B*Z_D
      
      
      
      mean = (1 - gate) * self.input_to_mean(input_) + gate * proposed_mean #B*Z_D
      # mean = gate * proposed_mean #B*Z_D
      
      _scale=F.elu(self.input_to_hid_scale(input_)) #B*F-->B*H
    
      for layer in self.hidden_layers:
        _scale=layer(_scale)     #B*H
      
      scale=F.softplus(self.hidden_to_scale(_scale))   #B*Z_D
                        
      return mean, scale  #B*Z_Dim both

class GatedTransition(nn.Module):
    """
    Parameterizes the gaussian latent transition probability `p(z_t | z_{t-1},u_{t-1})`
    See section 5 in the reference for comparison.
    """
    def __init__(self, z_dim,u_dim ,trans_dim):

        super(GatedTransition, self).__init__()

        self.gate = nn.Sequential( 
            nn.Linear(z_dim+u_dim, trans_dim),
            nn.ReLU(),
            nn.Linear(trans_dim, z_dim),
            nn.Sigmoid()
        )

        self.proposed_mean = nn.Sequential(
            nn.Linear(z_dim+u_dim, trans_dim),
            nn.ReLU(),
            nn.Linear(trans_dim, z_dim)
        ) 

        self.z_to_mu = nn.Linear(z_dim, z_dim)
        # modify the default initialization of z_to_mu so that it starts out as the identity function
        self.z_to_mu.weight.data = torch.eye(z_dim)
        self.z_to_mu.bias.data = torch.zeros(z_dim)
        self.z_to_logvar = nn.Linear(z_dim, z_dim)
        self.relu = nn.ReLU()

    def forward(self, z_t_1,u_t_1):
        """
        Given the latent `z_{t-1}` corresponding to the time step t-1
        we return the mean and scale vectors that parameterize the (diagonal) gaussian distribution `p(z_t | z_{t-1})`
        u_t_1 is the action
        z_t_1 has shape B*Z_d
        u_t_1 has shape B*T_d
        """
        input_=torch.cat([z_t_1,u_t_1],axis=1)        
        
        gate = self.gate(input_) # compute the gating function  #B*Z_d

        proposed_mean = self.proposed_mean(input_) #B*(Z_d+T_d)---->B*Z_d
        
        # compute the scale used to sample z_t, using the proposed mean from
        mu = (1 - gate) * self.z_to_mu(z_t_1) + gate * proposed_mean #everything B*Z_d
       
        
        logvar = self.z_to_logvar(self.relu(proposed_mean))         
        epsilon = torch.randn(z_t_1.size(), device=z_t_1.device) # sampling z by re-parameterization
        
        z_t = mu + epsilon * torch.exp(0.5 * logvar)    
        
        return z_t, mu, logvar   #Batch_size*Z_dim all three

"""### Encoder"""

class Encoder(nn.Module):
  """ Encodes x_{:t} for the variational distribution
     adopted  from https://github.com/guxd/deepHMM/blob/master/modules.py#L163
  """

  def __init__(self,input_dim,hidden_dim,n_layers, dropout=0.0, noise_radius=0.2):
      super(Encoder,self).__init__()
      self.rnn=nn.GRU(input_dim,hidden_dim,n_layers,batch_first=True)
      self.dropout=dropout
      self.noise_radius=noise_radius
      self.n_layers=n_layers
      self.hidden_dim=hidden_dim
      
      self.init_h = nn.Parameter(torch.randn(n_layers,1,
                                             hidden_dim), requires_grad=True)
      self.init_weights()

  def  init_weights(self):
        for w in self.rnn.parameters(): # initialize the gate weights with orthogonal
            if w.dim()>1:
                weight_init.orthogonal_(w)

  def forward(self,obs,obs_lens, init_h=None, noise=False):

    """
    obs: A mini batch of observations B*T*D
    obs_lens=observation lengths to pack pad sequences

    """
    batch_size, max_len, freq=obs.size()
    obs=F.dropout(obs,training=self.training)  #B*T*D
    
    obs_lens=torch.LongTensor(obs_lens).to(device)
    obs_lens_sorted, indices = obs_lens.sort(descending=True)
    obs_sorted = obs.index_select(0, indices)  
    
    packed_obs=pack_padded_sequence(obs_sorted,obs_lens_sorted.data.tolist(),batch_first=True)

    if init_h is None:
        init_h = self.init_h.expand(-1,batch_size,-1).contiguous()  

    hids, h_n = self.rnn(packed_obs, init_h) # hids: [B x T x H]  
                                                  # h_n: [num_layers*B*H)
    _, inv_indices = indices.sort()
    hids, lens = pad_packed_sequence(hids, batch_first=True)     
    hids = hids.index_select(0, inv_indices)
    h_n = h_n.index_select(1, inv_indices)

    h_n = h_n.view(self.n_layers,1, batch_size, self.hidden_dim) #[n_layers x n_dirs x batch_sz x hid_sz]
    h_n = h_n[-1] # get the last layer [n_dirs x batch_sz x hid_sz]
    enc = h_n.transpose(0,1).contiguous().view(batch_size,-1) #[batch_sz x (n_dirs*hid_sz)]
     
    if noise and self.noise_radius > 0:
         gauss_noise = torch.normal(means=torch.zeros(enc.size(), device=inputs.device),std=self.noise_radius)
         enc = enc + gauss_noise
            
    return enc, hids

class PostNet(nn.Module):
    """
    Parameterizes `q(z_t|z_{t-1}, x_{:t})`, which is the basic building block of the inference (i.e. the variational distribution). 
    The dependence on `x_{:t}` is through the hidden state of the RNN
    """
    def __init__(self, z_dim, h_dim):
        super(PostNet, self).__init__()
        self.z_to_h = nn.Sequential(
            nn.Linear(z_dim, h_dim),
            nn.Tanh()
        )
        self.h_to_mu = nn.Linear(h_dim, z_dim)
        self.h_to_logvar = nn.Linear(h_dim, z_dim)

    def forward(self, z_t_1, h_x):
        """
        Given the latent z at a particular time step t-1 as well as the hidden
        state of the RNN `h(x_{t:T})` we return the mean and scale vectors that
        parameterize the (diagonal) gaussian distribution `q(z_t|z_{t-1}, x_{t:T})`
        """
        # combine the rnn hidden state with a transformed version of z_t_1
        h_combined = 0.5*(self.z_to_h(z_t_1) + h_x)   #B*H
        mu = self.h_to_mu(h_combined)   #B*Z_d
        
        logvar = self.h_to_logvar(h_combined)  #B*Z_d
        std = torch.exp(0.5 * logvar)       #B*Z_d 
        
        epsilon = torch.randn(z_t_1.size(), device=z_t_1.device) # sampling z by re-parameterization        
        z_t = epsilon * std + mu   # [B*Z_d]
        
        return z_t, mu, logvar






"""### Main DMM Class"""

cofig={'x_dim':12,'binary_dim':None,'u_dim':2,'z_dim':100,'trans_dim':64,'rnn_dim':
        512,'clip_norm':12,'rnn_layers':1,'hidden_dim':64}

class DMM(nn.Module):

  """
  Main class implementening the Deep Markov Model
  Adapted from   
   https://pyro.ai/examples/dmm.html
   https://github.com/guxd/deepHMM/blob/master/models/dhmm.py

  """
  def __init__(self,z_dim,x_dim,u_dim,config):
    """
    config is a dict
    """
    super(DMM,self).__init__()

    self.binary=config['binary_dim']
    self.x_dim = config['x_dim']
    self.u_dim=config['u_dim']
    self.z_dim = config['z_dim']
    self.trans_dim = config['trans_dim']
    self.rnn_dim = config['rnn_dim']
    self.clip_norm = config['clip_norm']
    self.rnn_layers=config=['rnn_layers']
    self.hidden_dim=config['hidden_dim']

    self.Emitter=Emitter(self.z_dim,self.x_dim)
    self.trans=GatedTransition(self.z_dim,self.u_dim,self.trans_dim)
    self.rnn=Encoder(self.x_dim,self.rnn_dim,self.rnn_layers)
    
    self.postnet=PostNet(self.z_dim,self.hidden_dim)

    # define a (trainable) parameters z_0 and z_q_0 that help define the probability distributions p(z_1) and q(z_1)
    # (since for t = 1 there are no previous latents to condition on)
    self.z_0 = nn.Parameter(torch.zeros(self.z_dim))
    self.z_q_0 = nn.Parameter(torch.zeros(self.z_dim))
    self.h_0 = nn.Parameter(torch.zeros(1, 1, self.rnn_dim)) 

    def kl_div(self, mu1, logvar1, mu2=None, logvar2=None):
       """
       Calculates the Analytic KL divergence between two Gaussians with diagonal covaraince
       """
       one = torch.ones(1, device=mu1.device)
       if mu2 is None: mu2=torch.zeros(1, device=mu1.device)
       if logvar2 is None: logvar2=torch.zeros(1, device=mu1.device)
       return torch.sum(0.5*(logvar2-logvar1+(torch.exp(logvar1)+(mu1-mu2).pow(2))/torch.exp(logvar2)-one), 1)    

    def infer(self,x,x_mask,u,x_lens):
        """
        infer q(z_{1:T}|x_{1:T}) (i.e. the variational distribution)
        ignore any binary observables for now
        """
        batch_size, _, x_dim = x.size()  #x has shape B*L*X_d
        
        T_max = max(x_lens)
        """
         Change this if you want to add more rnn layers
        """
        h_0 = self.h_0.expand(1, batch_size, self.rnn.hidden_size).contiguous()
        
        _, rnn_out = self.rnn(x_rev, x_lens, h_0) # push the observed x's through the rnn;
        
        rec_losses = torch.zeros((batch_size, T_max), device=x.device) 
        kl_states = torch.zeros((batch_size, T_max), device=x.device)  
        
        # set z_prev=z_q_0 to setup the recursive conditioning in q(z_t|...)
        z_prev = self.z_q_0.expand(batch_size, self.z_q_0.size(0)) #B*Z_d

        # Set initial u (actions) to 0
        u_prev=torch.zeros((u.size[0],u.size[1])).to(device)  #Shape : B*U_d
        
        for t in range(T_max):
            
            z_prior, z_prior_mu, z_prior_logvar = self.trans(z_prev,u_prev)# p(z_t| z_{t-1},u_{t-1})            
            z_t, z_mu, z_logvar = self.postnet(z_prev, rnn_out[:,t,:])   #q(z_t | z_{t-1}, x_{t:T})
            
            # kl = self.kl_div(z_mu, z_logvar, z_prior_mu, z_prior_logvar)
            kl_states[:,t] = self.kl_div(z_mu, z_logvar, z_prior_mu, z_prior_logvar)
            
            x_t,x_mu, x_logvar=self.Emitter(z_t)  #x_t should have shape B*X_d                
            rec_loss=nn.MSELoss(reduction='none')(x_t.view(-1).contigous(),
                                                               x[:,t,:].contiguous().view(-1)).view(batch_size, -1)
            rec_losses[:,t] = rec_loss.mean(dim=1)       
            
            z_prev = z_t
            u_prev=u[:,t,:]   
        
        
        rec_loss = rec_losses.view(-1).masked_select(x_mask).mean()
        kl_loss = kl_states.view(-1).masked_select(x_mask).mean()  
        
        return rec_loss, kl_loss    

    def train_AE(self,x,x_mask,u,x_lens,kl_anneal):
        
        self.Emitter.train()
        self.trans.train()
        self.rnn.train()
        self.postnet.train()
               
        rec_loss, kl_loss = self.infer(x,x_mask,u,x_lens)
        loss = rec_loss + kl_anneal*kl_loss
        self.optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(self.parameters(), self.clip_norm)
        
        self.optimizer.step()
        
        return {'train_loss_AE':loss.item(), 'train_loss_KL':kl_loss.item()}
    
    def valid(self,x, x_mask,u, x_lens):
        self.Emitter.eval()
        self.trans.eval()
        self.rnn.eval()
        self.postnet.eval()
        
        rec_loss, kl_loss = self.infer(x,x_mask,u,x_lens)
        loss = rec_loss + kl_loss
        return loss
    
    def generate(self,x, x_mask,u, x_lens):
        """
        generation model p(x_{1:T} | z_{1:T}) p(z_{1:T})
        """
        batch_size, _, x_dim = x.size() 
        T_max = max(x_lens)
        
        # set z_prev=z_0 to setup the recursive conditioning in p(z_t|z_{t-1}
        z_prev = self.z_0.expand(batch_size, self.z_0.size(0))
        u_prev=torch.zeros((u.size[0],u.size[1])).to(device)  #Shape : B*U_d
        
        x=[]
        for t in range(1, T_max + 1):
            # sample z_t ~ p(z_t | z_{t-1}) one time step at a time
            z_t, z_mu, z_logvar = self.trans(z_prev,u_prev) 
            x_t,mu, logvar =self.Emitter(z_prev,u_prev)

            x.append(x_t)              
            z_prev = z_t
            u_prev=u[:,t,:]



