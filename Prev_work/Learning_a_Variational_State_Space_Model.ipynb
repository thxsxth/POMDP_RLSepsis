{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learning a Variational State Space Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thxsxth/RLMimic/blob/master/Learning_a_Variational_State_Space_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAsw49_5KTLv",
        "colab_type": "text"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UYC6LDM7BAq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "6472ca39-5001-4e86-e044-2d6532fa8a56"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QntEaWJOs1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd 'drive/My Drive/sepsis3-cohort'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8tbgRQNqNGF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "8146fe4f-6742-4291-efb4-32f6c4c8d232"
      },
      "source": [
        "!pip install pyro-ppl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyro-ppl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/77/4db4946f6b5bf0601869c7b7594def42a7197729167484e1779fff5ca0d6/pyro_ppl-1.3.1-py3-none-any.whl (520kB)\n",
            "\u001b[K     |████████████████████████████████| 522kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.18.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.2.1)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.5.1+cu101)\n",
            "Collecting pyro-api>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/81/957ae78e6398460a7230b0eb9b8f1cb954c5e913e868e48d89324c68cec7/pyro_api-0.1.2-py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->pyro-ppl) (0.16.0)\n",
            "Installing collected packages: pyro-api, pyro-ppl\n",
            "Successfully installed pyro-api-0.1.2 pyro-ppl-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8MuB_VsDBYh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a8e7d4e5-682e-47d4-daec-ec85b64a4210"
      },
      "source": [
        "# !pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (49.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBcFGXolZgIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import random\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import glob\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence,pad_packed_sequence\n",
        "import torch.nn.init as weight_init\n",
        "import scipy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ1hZH9oVKMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "import pyro.poutine as poutine\n",
        "from pyro.distributions import TransformedDistribution\n",
        "from pyro.distributions.transforms import affine_autoregressive\n",
        "from pyro.infer import SVI, JitTrace_ELBO, Trace_ELBO, TraceEnum_ELBO, TraceTMC_ELBO, config_enumerate,TraceGraph_ELBO\n",
        "from pyro.optim import ClippedAdam\n",
        "from utils import *\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0fZ2OLLPJdF",
        "colab_type": "text"
      },
      "source": [
        "### Some Data Cleaning\n",
        "\n",
        "We have pivoted Vitals, Sofa related scores and labs in CSV files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kKP6KFrYK6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vitals=pd.read_csv('../Vitals/Vitals.csv',parse_dates=['charttime']) #pivoted vitals\n",
        "sofa=pd.read_csv('../pivoted_sofa/pivoted_sofa.csv',parse_dates=['endtime','starttime']) #pivoted sofa\n",
        "labs=pd.read_csv('../pivoted_labs/Pivoted_labs.csv',parse_dates=['charttime'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s2YA8OofRKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sofa['Vaso']=sofa['rate_norepinephrine'].add(0.1*sofa['rate_dopamine'],fill_value=0).add(sofa['rate_epinephrine'],fill_value=0)\n",
        "sofa['Vaso']=sofa['Vaso'].add(sofa['rate_dobutamine'],fill_value=0).fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mExIHRWzLD-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vitals['TempC']=vitals['TempC'].ffill()\n",
        "sofa['GCS_min']=sofa['GCS_min'].ffill()\n",
        "labs['icustay_id']=labs['ICUSTAY_ID']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D-a3m2JPk5X",
        "colab_type": "text"
      },
      "source": [
        "The following, takes the cohort information and re indexes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48ywR78iLFEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "co=pd.read_csv('sepsis3_adults.csv',\n",
        "               parse_dates=['intime','outtime','suspected_infection_time_poe']) #cohort + demographics\n",
        "co=co.set_index('icustay_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mimEfBbP0jn",
        "colab_type": "text"
      },
      "source": [
        "Takes the deathtime from admissions table and include it in the Cohort"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJKPrv3NTQxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "admissions=pd.read_csv('admissions.csv',parse_dates=['ADMITTIME','DISCHTIME','DEATHTIME'])\n",
        "admissions=admissions.set_index('icustay_id').sort_index()\n",
        "co['death_time']=admissions['DEATHTIME']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl-mwWjRQLFk",
        "colab_type": "text"
      },
      "source": [
        "##### Input comes in two seperate Databases, the following combines them together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOGJejdQf7EF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_cv=pd.read_csv('../Fluids/cleaned_input_cv.csv',parse_dates=['charttime']) \n",
        "input_mv=pd.read_csv('../Fluids/input_eventsMV.csv',parse_dates=['starttime','endtime'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_NqIDKmf9rk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_cv=input_cv[['icustay_id','charttime','tev']]\n",
        "input_mv=input_mv[['icustay_id','endtime','tev']]\n",
        "input_mv['tev_mv']=input_mv['tev']\n",
        "input_mv['charttime']=input_mv['endtime']\n",
        "input_mv=input_mv.drop('tev',axis=1)\n",
        "input_fluids=input_mv.merge(input_cv,on=['icustay_id','charttime'],how='outer')[['icustay_id','charttime','tev','tev_mv']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLZDPn13gC0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_fluids['volume']=input_fluids['tev'].add(input_fluids['tev_mv'],fill_value=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6Z9LlFcQaKS",
        "colab_type": "text"
      },
      "source": [
        "### Normalizing all the data\n",
        "Means were precomputed, but can be easily derived from df.describe() or df.loc[].mean()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ioddtlYgDfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_fluids['volume']=(input_fluids['volume']-7.508081e+01)/5.910144e+02\n",
        "input_fluids=input_fluids.drop(['tev','tev_mv'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCgeWm5OTAzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vital_means=pd.Series([1.014094e+02,1.215081e+02, 6.027904e+01 ,7.915051e+01 ,2.020732e+01,3.689988e+01,9.708047e+01],\n",
        "                      index=['HeartRate','SysBP','DiasBP'\t,'MeanBP'\t,'RespRate'\t,'TempC','SpO2'])\n",
        "\n",
        "std_vitals=pd.Series([3.279276e+01,2.368822e+01,1.455537e+01,1.673107e+01,6.043442e+00,7.760427e-01,3.495480e+00],index=['HeartRate','SysBP','DiasBP'\t,'MeanBP'\t,'RespRate'\t,'TempC','SpO2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QxwRmDsUbDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Normalize Vitals\n",
        "for vital in vital_means.index:\n",
        "   vitals[vital]=(vitals[vital]-vital_means[vital])/std_vitals[vital]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgJYw9sX0JtH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "015ffca7-5969-4f43-8c5b-bc822d625845"
      },
      "source": [
        "sofa.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>icustay_id</th>\n",
              "      <th>hr</th>\n",
              "      <th>PaO2FiO2Ratio_novent</th>\n",
              "      <th>PaO2FiO2Ratio_vent</th>\n",
              "      <th>rate_epinephrine</th>\n",
              "      <th>rate_norepinephrine</th>\n",
              "      <th>rate_dopamine</th>\n",
              "      <th>rate_dobutamine</th>\n",
              "      <th>MeanBP_min</th>\n",
              "      <th>GCS_min</th>\n",
              "      <th>urineoutput</th>\n",
              "      <th>bilirubin_max</th>\n",
              "      <th>creatinine_max</th>\n",
              "      <th>platelet_min</th>\n",
              "      <th>respiration</th>\n",
              "      <th>coagulation</th>\n",
              "      <th>liver</th>\n",
              "      <th>cardiovascular</th>\n",
              "      <th>cns</th>\n",
              "      <th>renal</th>\n",
              "      <th>respiration_24hours</th>\n",
              "      <th>coagulation_24hours</th>\n",
              "      <th>liver_24hours</th>\n",
              "      <th>cardiovascular_24hours</th>\n",
              "      <th>cns_24hours</th>\n",
              "      <th>renal_24hours</th>\n",
              "      <th>SOFA_24hours</th>\n",
              "      <th>Vaso</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5.380769e+06</td>\n",
              "      <td>5.380769e+06</td>\n",
              "      <td>9174.000000</td>\n",
              "      <td>286131.000000</td>\n",
              "      <td>43579.000000</td>\n",
              "      <td>335605.000000</td>\n",
              "      <td>91099.000000</td>\n",
              "      <td>34683.000000</td>\n",
              "      <td>4.615092e+06</td>\n",
              "      <td>5.380768e+06</td>\n",
              "      <td>3.178315e+06</td>\n",
              "      <td>77091.000000</td>\n",
              "      <td>333873.000000</td>\n",
              "      <td>314318.00000</td>\n",
              "      <td>295305.000000</td>\n",
              "      <td>314318.000000</td>\n",
              "      <td>77091.00000</td>\n",
              "      <td>4.636356e+06</td>\n",
              "      <td>1.508665e+06</td>\n",
              "      <td>4.732542e+06</td>\n",
              "      <td>5.380769e+06</td>\n",
              "      <td>5.380769e+06</td>\n",
              "      <td>5.380769e+06</td>\n",
              "      <td>5.380769e+06</td>\n",
              "      <td>5.380769e+06</td>\n",
              "      <td>5.380769e+06</td>\n",
              "      <td>5.380769e+06</td>\n",
              "      <td>5.380769e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.498804e+05</td>\n",
              "      <td>1.580930e+02</td>\n",
              "      <td>252.089197</td>\n",
              "      <td>249.689558</td>\n",
              "      <td>0.051119</td>\n",
              "      <td>0.177287</td>\n",
              "      <td>6.320698</td>\n",
              "      <td>4.878103</td>\n",
              "      <td>7.817685e+01</td>\n",
              "      <td>1.177083e+01</td>\n",
              "      <td>1.687066e+02</td>\n",
              "      <td>4.012415</td>\n",
              "      <td>1.545177</td>\n",
              "      <td>217.11330</td>\n",
              "      <td>1.173150</td>\n",
              "      <td>0.596005</td>\n",
              "      <td>1.13021</td>\n",
              "      <td>5.897353e-01</td>\n",
              "      <td>1.467729e+00</td>\n",
              "      <td>4.285035e-01</td>\n",
              "      <td>6.920319e-01</td>\n",
              "      <td>4.702023e-01</td>\n",
              "      <td>2.968944e-01</td>\n",
              "      <td>1.055728e+00</td>\n",
              "      <td>1.805929e+00</td>\n",
              "      <td>1.282797e+00</td>\n",
              "      <td>5.603582e+00</td>\n",
              "      <td>5.361584e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.878473e+04</td>\n",
              "      <td>2.377073e+02</td>\n",
              "      <td>145.262215</td>\n",
              "      <td>115.511308</td>\n",
              "      <td>0.218740</td>\n",
              "      <td>0.493985</td>\n",
              "      <td>6.742377</td>\n",
              "      <td>3.595475</td>\n",
              "      <td>1.602817e+01</td>\n",
              "      <td>3.691253e+00</td>\n",
              "      <td>2.621426e+03</td>\n",
              "      <td>7.413100</td>\n",
              "      <td>1.550674</td>\n",
              "      <td>141.69615</td>\n",
              "      <td>1.506095</td>\n",
              "      <td>0.928538</td>\n",
              "      <td>1.35431</td>\n",
              "      <td>1.018574e+00</td>\n",
              "      <td>1.368434e+00</td>\n",
              "      <td>1.135888e+00</td>\n",
              "      <td>1.294751e+00</td>\n",
              "      <td>8.540253e-01</td>\n",
              "      <td>8.471924e-01</td>\n",
              "      <td>1.034393e+00</td>\n",
              "      <td>1.453541e+00</td>\n",
              "      <td>1.532017e+00</td>\n",
              "      <td>3.798351e+00</td>\n",
              "      <td>5.292760e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.000010e+05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>2.000000e-01</td>\n",
              "      <td>3.000000e+00</td>\n",
              "      <td>-1.440000e+04</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>5.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.248890e+05</td>\n",
              "      <td>2.600000e+01</td>\n",
              "      <td>138.571429</td>\n",
              "      <td>168.333327</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.056140</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>6.700000e+01</td>\n",
              "      <td>1.000000e+01</td>\n",
              "      <td>4.500000e+01</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>119.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.499450e+05</td>\n",
              "      <td>7.000000e+01</td>\n",
              "      <td>228.000000</td>\n",
              "      <td>232.000000</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>0.101587</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.006904</td>\n",
              "      <td>7.600000e+01</td>\n",
              "      <td>1.400000e+01</td>\n",
              "      <td>1.000000e+02</td>\n",
              "      <td>1.100000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>189.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>5.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.747130e+05</td>\n",
              "      <td>1.930000e+02</td>\n",
              "      <td>346.000000</td>\n",
              "      <td>312.000000</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.201159</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>5.046614</td>\n",
              "      <td>8.766670e+01</td>\n",
              "      <td>1.500000e+01</td>\n",
              "      <td>1.900000e+02</td>\n",
              "      <td>3.800000</td>\n",
              "      <td>1.700000</td>\n",
              "      <td>279.00000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>3.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>3.000000e+00</td>\n",
              "      <td>3.000000e+00</td>\n",
              "      <td>8.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2.999990e+05</td>\n",
              "      <td>4.154000e+03</td>\n",
              "      <td>2433.333333</td>\n",
              "      <td>1929.166710</td>\n",
              "      <td>19.244444</td>\n",
              "      <td>61.439027</td>\n",
              "      <td>523.784242</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>2.990000e+02</td>\n",
              "      <td>1.500000e+01</td>\n",
              "      <td>4.555555e+06</td>\n",
              "      <td>82.800000</td>\n",
              "      <td>138.000000</td>\n",
              "      <td>2292.00000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.00000</td>\n",
              "      <td>4.000000e+00</td>\n",
              "      <td>4.000000e+00</td>\n",
              "      <td>4.000000e+00</td>\n",
              "      <td>4.000000e+00</td>\n",
              "      <td>4.000000e+00</td>\n",
              "      <td>4.000000e+00</td>\n",
              "      <td>4.000000e+00</td>\n",
              "      <td>4.000000e+00</td>\n",
              "      <td>4.000000e+00</td>\n",
              "      <td>2.400000e+01</td>\n",
              "      <td>6.143903e+01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         icustay_id            hr  ...  SOFA_24hours          Vaso\n",
              "count  5.380769e+06  5.380769e+06  ...  5.380769e+06  5.380769e+06\n",
              "mean   2.498804e+05  1.580930e+02  ...  5.603582e+00  5.361584e-02\n",
              "std    2.878473e+04  2.377073e+02  ...  3.798351e+00  5.292760e-01\n",
              "min    2.000010e+05  0.000000e+00  ...  0.000000e+00  0.000000e+00\n",
              "25%    2.248890e+05  2.600000e+01  ...  3.000000e+00  0.000000e+00\n",
              "50%    2.499450e+05  7.000000e+01  ...  5.000000e+00  0.000000e+00\n",
              "75%    2.747130e+05  1.930000e+02  ...  8.000000e+00  0.000000e+00\n",
              "max    2.999990e+05  4.154000e+03  ...  2.400000e+01  6.143903e+01\n",
              "\n",
              "[8 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy20HN4Z0VoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sofa_means=pd.Series([5.361584e-02, 2.968944e-01,1.055728e+00,1.805929e+00,1.282797e+00,5.603582e+00],\n",
        "                      index=['Vaso','liver_24hours'\t,'cardiovascular_24hours','cns_24hours'\t,'renal_24hours',\t'SOFA_24hours'])\n",
        "\n",
        "sofa_stds=pd.Series([5.292760e-01, 8.471924e-01,1.034393e+00,1.453541e+00,1.532017e+00,3.798351e+00],index=['Vaso','liver_24hours','cardiovascular_24hours','cns_24hours'\t,'renal_24hours',\t'SOFA_24hours'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGuAg__sZ6mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  sofa_means=pd.Series([0.619876, 2.968944e-01,1.055728e+00,1.805929e+00,1.282797e+00,5.603582e+00],\n",
        "#                       index=['Vaso','liver_24hours'\t,'cardiovascular_24hours','cns_24hours'\t,'renal_24hours',\t'SOFA_24hours'])\n",
        "\n",
        "# sofa_stds=pd.Series([1.699333, 8.471924e-01,1.034393e+00,1.453541e+00,1.532017e+00,3.798351e+00],index=['Vaso','liver_24hours','cardiovascular_24hours','cns_24hours'\t,'renal_24hours',\t'SOFA_24hours'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48Fli_GzVL0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for sofa_ in sofa_means.index:\n",
        "   sofa[sofa_]=(sofa[sofa_]-sofa_means[sofa_])/sofa_stds[sofa_]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N159DsUdObm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sofa['Vaso']=sofa['Vaso'].fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnQOBzPbheVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "co['age']=(co['age']-66.052869)/16.179046\t\n",
        "co['Weight']=(co['Weight']-81.135925)/24.059419\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW5p9BU4Qvtw",
        "colab_type": "text"
      },
      "source": [
        "### Dataset and Dataloader\n",
        "We subclass PyTorch Dataset class to access patient level trajectories. Then we use a customized collate function to batch into minibatches after padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pfbkiiTcjab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mini_batch_mask(mini_batch, seq_lengths):\n",
        "    mask = torch.zeros(mini_batch.shape[0:2])\n",
        "    for b in range(mini_batch.shape[0]):\n",
        "        mask[b, 0:seq_lengths[b]] = torch.ones(seq_lengths[b])\n",
        "    return mask.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jBveF_Y5n9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class modeling_dataset(Dataset):\n",
        "  \"\"\"\n",
        "  Implements a dataset for patients\n",
        "  Needs Vitals,Sofa,Inputs,co tables\n",
        "\n",
        "  This is only used for Model Learning, Rewards, Terminals are unnecessary\n",
        "  \"\"\"\n",
        "  def __init__(self,patient_ids,train=True):\n",
        "    #patient_ids :List/np.array\n",
        "    self.ids=patient_ids\n",
        "    self.train=train\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ids)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    #Get Patient from the index\n",
        "    pat=self.ids[idx]\n",
        "    pat_fluids=input_fluids[input_fluids.icustay_id==pat].set_index('charttime')\n",
        "    pat_sofa=sofa[sofa.icustay_id==pat].set_index('endtime')\n",
        "    pat_sofa=pd.concat([pat_sofa,pat_fluids]).resample('H').sum()\n",
        "\n",
        "    pat_vitals=vitals[vitals.icustay_id==pat].set_index('charttime')\n",
        "    pat_labs=labs[labs.icustay_id==pat]\n",
        "    pat_df=pd.concat([pat_vitals,\n",
        "                              pat_sofa]).resample('H').last()[['HeartRate','SysBP','DiasBP',\t'MeanBP','RespRate','SpO2','TempC',\n",
        "                                      'liver_24hours','cardiovascular_24hours',\n",
        "                                      'cns_24hours','renal_24hours','SOFA_24hours','volume','Vaso']].resample('H').last()\n",
        "    # pat_df=pd.concat([pat_vitals,\n",
        "    #                           pat_sofa]).resample('H').last()[['HeartRate','SysBP','DiasBP',\t'MeanBP','RespRate','SpO2',\n",
        "    #                                   'volume','Vaso']].resample('H').last()\n",
        "  \n",
        "    \"\"\"\n",
        "    TO DO: \n",
        "    Implement get_rewards\n",
        "    Get age gender and if they died may be weight height\n",
        "    GET TREATMENTS: Make it Tensor DONE\n",
        "    GET Trajectory Make it Tensor DONE\n",
        "    \"\"\"\n",
        "    dead=co.loc[pat].HOSPITAL_EXPIRE_FLAG==1\n",
        "    if co.loc[pat].HOSPITAL_EXPIRE_FLAG==1:\n",
        "          pat_df=pat_df.truncate(after=co.loc[pat].death_time)\n",
        "\n",
        "    pat_df=pat_df.ffill().dropna()\n",
        "    # print(pat_df.shape)\n",
        "\n",
        "    constants=torch.FloatTensor(co.loc[pat][['age','is_male','Weight']]).to(device)\n",
        "\n",
        "    if pat_df.shape[0]>47:\n",
        "          k=np.random.choice(pat_df.shape[0]-47)\n",
        "          pat_df=pat_df.iloc[k:k+47,:]\n",
        "\n",
        "    \n",
        "    treatments=torch.FloatTensor(pat_df[['Vaso','volume']].values).to(device)     \n",
        "    trajectory=torch.FloatTensor(pat_df.drop(['Vaso','volume'],axis=1).values).to(device)\n",
        "\n",
        "    return trajectory,treatments, constants\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr3QZlqZ51cl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_model(batch_data):\n",
        "\n",
        "  \"\"\"\n",
        "  We will be a list of tuples,\n",
        "  len(list) will be batch_size\n",
        "  (trajectory,treatments, constants,dead) for each patient in batch\n",
        "  \"\"\"\n",
        "  trajectories=[]\n",
        "  treatments=[]\n",
        "  seq_lens=[]\n",
        "  constants_list=[]\n",
        "  dead_=[]\n",
        "\n",
        "  for (trajectory,treatment,constants) in batch_data:\n",
        "\n",
        "    trajectories.append(trajectory) \n",
        "    treatments.append(treatment)\n",
        "    seq_lens.append(trajectory.shape[0])\n",
        "    constants_list.append(constants.unsqueeze(0))\n",
        "    # dead_.append(dead)\n",
        "\n",
        "  padded_trajectories=pad_sequence(trajectories,batch_first=True)\n",
        "  padded_treatments=pad_sequence(treatments,batch_first=True)\n",
        "  constants=torch.cat(constants_list)\n",
        "  mask=get_mini_batch_mask(padded_trajectories,seq_lens)\n",
        "\n",
        "  return padded_trajectories,padded_treatments,constants,mask,seq_lens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6-Jrw_1RHo8",
        "colab_type": "text"
      },
      "source": [
        "#### The Training and Validation Cohorts. The Modelling Cohort was pre selected using previous work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzqKfGMkhJ4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "patient_list=list(np.load('new_pats.npy'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_oYGL9ZhQKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_cohort=np.random.choice(a=patient_list, size=int(0.8*len(patient_list)), replace=False, p=None)\n",
        "train_dataset=modeling_dataset(training_cohort)\n",
        "train_loader=DataLoader(train_dataset,batch_size=32,shuffle=True,collate_fn=collate_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx-MoAHVkphX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_cohort=list(set(patient_list)-set(training_cohort))\n",
        "valid_dataset=modeling_dataset(valid_cohort)\n",
        "valid_loader=DataLoader(valid_dataset,batch_size=16,shuffle=False,collate_fn=collate_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wroql9SiyVuE",
        "colab_type": "text"
      },
      "source": [
        "### Defining Models and hyperparamters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNgpzIjE1gZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class patient_encoder(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  Takes constant demographic details and possibly labs and encodes into  z_0, h_0\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,constant_dim,z_dim,h_dim,hidden_dim=64):\n",
        "    \n",
        "    super(patient_encoder,self).__init__()\n",
        "    self.linear_input=nn.Linear(constant_dim,hidden_dim)\n",
        "    self.h0=nn.Linear(hidden_dim,h_dim)\n",
        "    self.z_mu=nn.Linear(hidden_dim,z_dim)\n",
        "    self.z_sigma=nn.Linear(hidden_dim,z_dim)\n",
        "    \n",
        "    \n",
        "\n",
        "  def forward(self,x):\n",
        "    \"\"\"\n",
        "    X has shape batch_size*input_dim or num_particles*batch_size*input_dim\n",
        "    \"\"\"\n",
        "    x=F.elu(self.linear_input(x)) #batch_size*hidden_dim\n",
        "    h0=self.h0(x)\n",
        "    s0=self.z_mu(x)\n",
        "    # z_sigma=F.softplus(self.z_sigma(x))\n",
        "\n",
        "    return h0,s0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQTJyiO4C2l1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  \"\"\" Encodes x_{:t} for the variational distribution\n",
        "     adopted  from https://github.com/guxd/deepHMM/blob/master/modules.py#L163\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,obs_dim,h_dim,a_dim,n_layers=1, dropout=0.0, noise_radius=0.2):\n",
        "      \n",
        "      super(Encoder,self).__init__()\n",
        "      \n",
        "      self.rnn=nn.GRU(obs_dim,h_dim,n_layers,batch_first=True)\n",
        "      self.dropout=dropout\n",
        "      \n",
        "      self.noise_radius=noise_radius\n",
        "      \n",
        "      self.n_layers=n_layers\n",
        "      self.hidden_dim=h_dim\n",
        "      \n",
        "      # self.init_h = nn.Parameter(torch.randn(n_layers,1,\n",
        "      #                                        hidden_dim), requires_grad=True)\n",
        "      self.init_weights()\n",
        "\n",
        "  def  init_weights(self):\n",
        "        for w in self.rnn.parameters(): # initialize the gate weights with orthogonal\n",
        "            if w.dim()>1:\n",
        "                weight_init.orthogonal_(w)\n",
        "\n",
        "  def forward(self,obs,obs_lens,patient_demos,init_h=None, noise=False):\n",
        "\n",
        "    \"\"\"\n",
        "    obs: A mini batch of observations B*T*D\n",
        "    obs_lens=observation lengths to pack pad sequences\n",
        "    patient_demos: Batch of size B*pat_dim (=C)\n",
        "    \"\"\"\n",
        "    batch_size, max_len, freq=obs.size()\n",
        "\n",
        "    # obs=F.dropout(obs,training=self.training)  #B*T*D\n",
        "    \n",
        "    obs_lens=torch.LongTensor(obs_lens).to(device)\n",
        "    obs_lens_sorted, indices = obs_lens.sort(descending=True)\n",
        "    obs_sorted = obs.index_select(0, indices)  \n",
        "    \n",
        "    packed_obs=pack_padded_sequence(obs_sorted,obs_lens_sorted.data.tolist(),batch_first=True)\n",
        "\n",
        "    if init_h is None:\n",
        "        init_h,_=self.patient_encoder(patient_demos).unsqueeze(0)\n",
        "\n",
        "    init_h=init_h.unsqueeze(0) #1*B*H\n",
        "  \n",
        "    hids, h_n = self.rnn(packed_obs, init_h) # hids: [B x T x H]  \n",
        "                                                  # h_n: [num_layers*B*H)\n",
        "    _, inv_indices = indices.sort()\n",
        "\n",
        "    hids, lens = pad_packed_sequence(hids, batch_first=True)         \n",
        "    hids = hids.index_select(0, inv_indices) \n",
        "    \n",
        "      #[B*T*H]\n",
        "    # h_n = h_n.index_select(1, inv_indices)     #[1*T*H]\n",
        "    \n",
        "\n",
        "    # h_n = h_n.view(1, batch_size, self.hidden_dim) #[n_layers x n_dirs x batch_sz x hid_sz]\n",
        "    \n",
        "    # h_n = h_n[-1] # get the last layer [1 x B x H]\n",
        "    # enc = h_n.transpose(0,1).contiguous().view(batch_size,-1) #[B*H]\n",
        "     \n",
        "    # if noise and self.noise_radius > 0:\n",
        "    #      gauss_noise = torch.normal(means=torch.zeros(enc.size(), device=inputs.device),std=self.noise_radius)\n",
        "    #      enc = enc + gauss_noise\n",
        "            \n",
        "    return hids\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QARkUqe1DA5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Proposer(nn.Module):\n",
        "    \"\"\"\n",
        "    Parameterizes `q(z_t | a_{t-1}, o_{:t}, a_{t})` the proposal distribution\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, z_dim, h_dim,a_dim,constant_dim,n_layers=2,mu_layers=1,sigma_layers=1,hidden_dim=64):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.z_dim=z_dim\n",
        "        input_size=z_dim+h_dim+a_dim+constant_dim\n",
        "        self.bn=nn.BatchNorm1d(input_size)\n",
        "        self.linear_input=nn.Linear(input_size,hidden_dim)\n",
        "        self.hiddens=nn.ModuleList([nn.Sequential(nn.Linear(hidden_dim,hidden_dim),nn.ELU(),nn.BatchNorm1d(hidden_dim)) for j in range((mu_layers))])\n",
        "\n",
        "        self.hiddens_mu=nn.ModuleList([nn.Sequential(nn.Linear(hidden_dim,hidden_dim),nn.ELU(),nn.BatchNorm1d(hidden_dim)) for j in range((mu_layers))])\n",
        "        self.hiddens_sigma=nn.ModuleList([nn.Sequential(nn.Linear(hidden_dim,hidden_dim),nn.ELU()) for j in range((sigma_layers))])\n",
        "        \n",
        "        \n",
        "        self.mu_linear=nn.Linear(hidden_dim,z_dim)\n",
        "        self.sigma_linear=nn.Linear(hidden_dim,z_dim)\n",
        "        self.sigma_param=nn.Parameter(torch.ones(z_dim).to(device)*0.001)\n",
        "\n",
        "\n",
        "    def forward(self, z_t_1, h_t,a_t,constants):\n",
        "      \"\"\"\n",
        "      inputs have batch_size*dim_ or alternatively batch_size*K*dim_\n",
        "      where k is the number of particles for SMC\n",
        "\n",
        "      \"\"\"        batch_size=z_t_1.shape[0]\n",
        "        \n",
        "        input_=torch.cat([z_t_1,h_t,a_t,constants],dim=1)   #Batch_size*(z_dim_h_dim+a_time)=S\n",
        "        \n",
        "        \n",
        "        input_=self.bn(input_) #B*S\n",
        "       \n",
        "    \n",
        "        \n",
        "    \n",
        "        input_=self.linear_input(input_)\n",
        "        # print(input_.shape)  #B*K*H\n",
        "\n",
        "        for layer in self.hiddens:\n",
        "            input_=layer(input_)  #B*K*H\n",
        "\n",
        "        mu=input_\n",
        "        sigma=input_\n",
        "\n",
        "        for layer in self.hiddens_mu:\n",
        "          mu=layer(mu)  #B*K*H\n",
        "\n",
        "        mu=self.mu_linear(mu) #B*K*Z\n",
        "\n",
        "        for layer in self.hiddens_sigma:\n",
        "           sigma=self.sigma_linear(sigma) #B*K*Z\n",
        "\n",
        "        sigma=F.softplus(sigma)\n",
        "\n",
        "      \n",
        "        # sigma = self.sigma_param.expand(batch_size,self.z_dim) #B*Z_dim\n",
        "\n",
        "\n",
        "\n",
        "        return mu, sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUWe_-JYDGrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Emitter(nn.Module):\n",
        "  \"\"\"\n",
        "  A MLP representing  p(o_t|z_t)\n",
        "  \"\"\"\n",
        "  def __init__(self,z_dim,continous_dim,poison_dim=None,hidden_size=128,hidden_layers=2):\n",
        "    \n",
        "    super(Emitter,self).__init__()\n",
        "    \n",
        "    self.cont_dim=continous_dim\n",
        "    self.bn=nn.BatchNorm1d(z_dim)\n",
        "    self.linear=nn.Linear(z_dim,hidden_size)\n",
        "    self.hidden_layers=nn.ModuleList([nn.Sequential(nn.Linear(hidden_size,hidden_size),nn.ELU(),nn.BatchNorm1d(hidden_size)) for i in range(hidden_layers)])\n",
        "\n",
        "    self.linear_mu=nn.Linear(hidden_size,continous_dim)\n",
        "    self.linear_sigma=nn.Linear(hidden_size,continous_dim)\n",
        "    self.poison_dim=poison_dim\n",
        "    \n",
        "    if poison_dim:\n",
        "      self.linear_poison=nn.Linear(hidden_size,poison_dim)\n",
        "\n",
        "    self.sigma_param=nn.Parameter(torch.ones(self.cont_dim).to(device)*0.1)\n",
        "\n",
        "  def forward(self,z_t):\n",
        "    \"\"\"\n",
        "    z has dimensions #batch_size*num_particles*Z_dim\n",
        "    \"\"\"\n",
        "    # z_t=z_t.permute(0,2,1)\n",
        "    # z=self.linear(self.bn(z_t))\n",
        "    batch_size=z_t.shape[0]\n",
        "    z=self.linear(z_t)\n",
        "   \n",
        "    for layer in self.hidden_layers:\n",
        "          z=layer(z)\n",
        "    \n",
        "    mu=self.linear_mu(z)\n",
        "    sigma=F.softplus(self.linear_sigma(z))\n",
        "    if self.poison_dim:\n",
        "      poison=F(self.linear_poison(z))\n",
        "      return mu, sigma,poison\n",
        "\n",
        "    sigma = self.sigma_param.expand(batch_size,self.cont_dim) #B*Z_dim\n",
        "\n",
        "    return mu,sigma  #Batch_size*K*Obs_dim\n",
        "\n",
        "class Gated_Transition(nn.Module):\n",
        "    \"\"\"\n",
        "    Parameterizes the gaussian latent transition probability p(z_t | z_{t-1},a_{t-1},h_{t-1})\n",
        "    using a MLP, where z is latent and a is the action\n",
        "    Modified from https://pyro.ai/examples/dmm.html\n",
        "    \"\"\"\n",
        "    def __init__(self, z_dim,a_dim,h_dim,constant_dim, hidden_dim,n_layers=3):\n",
        "      super(Gated_Transition,self).__init__()\n",
        "      \n",
        "      input_dim=z_dim+a_dim+h_dim+constant_dim\n",
        "\n",
        "      self.bn=nn.BatchNorm1d(input_dim)\n",
        "      self.input_to_h=nn.Linear(input_dim, hidden_dim)\n",
        "      self.hidden_layers=nn.ModuleList([nn.Sequential(nn.Linear(hidden_dim,hidden_dim),nn.ELU(),nn.BatchNorm1d(hidden_dim)) for i in range(n_layers)])\n",
        "      self.hidden_to_z=nn.Linear(hidden_dim,z_dim)\n",
        "      \n",
        "      self.proposed_mean_to_h=nn.Linear(input_dim, hidden_dim)\n",
        "      self.proposed_h_to_z=nn.Linear(hidden_dim,z_dim)\n",
        "\n",
        "      self.input_to_mean=nn.Linear(input_dim,z_dim)\n",
        "      self.input_to_scale=nn.Linear(input_dim,z_dim)\n",
        "      self.input_to_hid_scale=nn.Linear(input_dim,hidden_dim)\n",
        "      self.hidden_to_scale=nn.Linear(hidden_dim,z_dim)\n",
        "\n",
        "      self.input_to_mean.bias.data = torch.zeros(z_dim)\n",
        "\n",
        "\n",
        "    def forward(self,z_prev,a_t,h_t,constants):\n",
        "  \n",
        "      \"\"\"\n",
        "        Given the latent z_{t-1} and the action u_{t-1}\n",
        "        \n",
        "        we return the mean and scale vectors that parameterize the\n",
        "        (diagonal) gaussian distribution p(z_t | z_{t-1},a_{t-1},h_{t-1})\n",
        "        \n",
        "        z_t_1 has shape B*num_particles*Z_dim or B*Z_dim\n",
        "        a_t_1 has shape B*num_particles*a_dim or B*a_dim\n",
        "      \"\"\"\n",
        "      # concatenate the latent z and actions along the frequency dimension\n",
        "      input_=torch.cat([z_prev,a_t,h_t,constants],dim=1)  #B*K*(D+T) let's call D+T=F\n",
        "      \n",
        "      # input_=self.bn(input_)\n",
        "\n",
        "      #calculate the gate\n",
        "      # Add hidden layers if necessary\n",
        "      \n",
        "      _gate=F.elu(self.input_to_h(input_))\n",
        "      gate=F.sigmoid(self.hidden_to_z(_gate))\n",
        "      \n",
        "\n",
        "      # compute the 'proposed mean'\n",
        "    \n",
        "      _proposed_mean = F.elu(self.proposed_mean_to_h(input_))  #B*K*U_D-->B*K*H_D   \n",
        "      for layer in self.hidden_layers:\n",
        "          _proposed_mean=layer(_proposed_mean)   #B*K*H_D\n",
        "    \n",
        "      proposed_mean = self.proposed_h_to_z(_proposed_mean)  #B*K*H_D--->B*K*Z_D\n",
        "      \n",
        "      \n",
        "      \n",
        "      mean = (1 - gate) * self.input_to_mean(input_) + gate * proposed_mean #B*K*Z_D\n",
        "      # mean = gate * proposed_mean #B*Z_D\n",
        "      \n",
        "      _scale=F.elu(self.input_to_hid_scale(input_)) #B*K*F-->B*K*H\n",
        "    \n",
        "      for layer in self.hidden_layers:\n",
        "        _scale=layer(_scale)     #B*K*H\n",
        "      \n",
        "      scale=F.softplus(self.hidden_to_scale(_scale))   #B*K*Z_D\n",
        "\n",
        "      \n",
        "                        \n",
        "      return mean,scale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1ReP_Bx61g1Z",
        "colab": {}
      },
      "source": [
        "class DMM(nn.Module):\n",
        "  \n",
        "  \"\"\"\n",
        "    This PyTorch Module encapsulates the model as well as the\n",
        "    variational distribution (the guide) for the Deep Markov Model\n",
        "\n",
        "    Modified from https://github.com/pyro-ppl/pyro/blob/dev/examples/dmm/dmm.py\n",
        "  \"\"\"\n",
        "  def __init__(self,z_dim,a_dim,obs_dim,constants_dim,h_dim,binary_dim=None,\n",
        "               hidden_emitter_dim=512,hidden_gated_dim=512,\n",
        "               ,num_iafs=0, iaf_dim=50):\n",
        "    \n",
        "    \"\"\"\n",
        "    z_dim (int) : Dimension of the Latent Space\n",
        "    a_dim (int) : Dimension of Action space\n",
        "    obs_dim (int) : Dimension of (conitnous) observations\n",
        "\n",
        "    binary_dim (int) : Dimension of binary observations\n",
        "    hidden_layers (iterable): Number of hidden layers for Emitter, transm Encoder respectively\n",
        "    Others should be self explanatory\n",
        "    \"\"\"\n",
        "    \n",
        "    super(DMM,self).__init__()\n",
        "    self.initial=patient_encoder(constants_dim,z_dim,h_dim)\n",
        "    self.emitter=Emitter(z_dim,obs_dim,binary_dim,hidden_emitter_dim)\n",
        "    self.trans=Gated_Transition(z_dim,a_dim,h_dim,constants_dim, hidden_dim=64)\n",
        "    self.proposer=Proposer(z_dim,h_dim,a_dim,constants_dim,n_layers=5,mu_layers=2,sigma_layers=1,hidden_dim=64)\n",
        "    self.rnn=Encoder(obs_dim,h_dim,a_dim,n_layers=1)\n",
        "    \n",
        "   \n",
        "    input_dim=z_dim+a_dim \n",
        "    if binary_dim:\n",
        "      self.rnn=Encoder(obs_dim+binary_dim,hidden_dim=h_dim,n_layers=hidden_layers[2])\n",
        "    else:\n",
        "      self.rnn=Encoder(obs_dim,h_dim,a_dim)\n",
        "    \n",
        "    # if we're using normalizing flows, instantiate those \n",
        "    self.iafs = [affine_autoregressive(z_dim, hidden_dims=[iaf_dim]) for _ in range(num_iafs)]\n",
        "    self.iafs_modules = nn.ModuleList(self.iafs)\n",
        "\n",
        "    if device=='cuda':\n",
        "      self.cuda()\n",
        "\n",
        "\n",
        "\n",
        "    def model(self,observations,obs_lens,actions,mask,constants,binary=None,\n",
        "               annealing_factor=1.0):\n",
        "      \"\"\"\n",
        "      batch : Batch of continous observables: B*T*O\n",
        "      binary: Batch of binary observables:\n",
        "      batch_lengths :list\n",
        "      actions : B*T*|A|\n",
        "      Constants B*C\n",
        "      \"\"\"\n",
        "      \n",
        "      T_max = observations.size(1)\n",
        "\n",
        "      # set z_prev = z_0 to setup the recursive conditioning in p(z_t | z_{t-1,s_{t-1}})\n",
        "      # and set initial treatment to zero\n",
        "      \n",
        "      z_prev,h_prev=self.initial(constants)  #B*Z_dim, B*h_dim\n",
        "      h_prev=h_prev.contiguous()\n",
        "\n",
        "      a_prev=torch.zeros(actions.shape[0],actions.shape[2]).to(device) #B*A\n",
        "      rnn_output=self.rnn(observations,obs_lens,constants,h_prev)  #rnn_ouput has shape B*T*H\n",
        "      \n",
        "                 \n",
        "      # we enclose all the sample statements in the model in a plate.\n",
        "      # this marks that each datapoint is conditionally independent of the others\n",
        "      \n",
        "      with pyro.plate(\"z_minibatch\", len(observations)):\n",
        "\n",
        "        for t in pyro.markov(range(1, T_max + 1)):\n",
        "\n",
        "          # the next chunk of code samples z_t ~ p(z_t | z_{t-1},u_{t-1})\n",
        "          # note that (both here and elsewhere) we use poutine.scale to take care of KL annealing\n",
        "\n",
        "          z_mean,z_scale=self.trans(z_prev,a_prev,rnn_output[:,t-1,:],constants)\n",
        "\n",
        "          with poutine.scale(scale=annealing_factor):              \n",
        "                    z_t = pyro.sample(\"z_%d\" % t,\n",
        "                                      dist.Normal(z_mean, z_scale)\n",
        "                                          .mask(mask[:, t - 1:t])\n",
        "                                          .to_event(1))\n",
        "          \n",
        "          if binary:\n",
        "            mu,sigma,binary=self.emitter(z_t)\n",
        "          else:            \n",
        "            mu, sigma=self.emitter(z_t)\n",
        "\n",
        "          pyro.sample(\"cts_x_%d\" % t,\n",
        "                            dist.Normal(mu,sigma)\n",
        "                                .mask(mask[:, t - 1:t])\n",
        "                                .to_event(1),\n",
        "                            obs=observations[:, t - 1, :])\n",
        "          \n",
        "          if binary:        \n",
        "               pyro.sample(\"binary_x_%d\" % t,\n",
        "                            dist.Bernoulli(binary)\n",
        "                                .mask(mask[:, t - 1:t])\n",
        "                                .to_event(1),\n",
        "                            obs=binary[:, t - 1, :])\n",
        "          z_prev = z_t\n",
        "          a_prev=actions[:,t-1,:]\n",
        "    \n",
        "    def guide(self,observations,obs_lens,actions,mask,constants,binary=None,\n",
        "               annealing_factor=1.0):\n",
        "\n",
        "      # this is the number of time steps we need to process in the mini-batch\n",
        "      T_max = observations.size(1)\n",
        "      # register all PyTorch (sub)modules with pyro\n",
        "      pyro.module(\"dmm\", self)\n",
        "\n",
        "       # if on gpu we need the fully broadcast view of the rnn initial state\n",
        "       # to be in contiguous gpu memory\n",
        "       \n",
        "      z_prev,h_prev=self.initial(constants)  #B*Z_dim, B*h_dim\n",
        "      h_prev=h_prev.contiguous()\n",
        "      rnn_output=self.rnn(observations,obs_lens,constants,h_prev)  #rnn_ouput has shape B*T*H\n",
        "       \n",
        "      a_prev=torch.zeros(actions.shape[0],actions.shape[2]).to(device)\n",
        "\n",
        "      with pyro.plate(\"z_minibatch\", len(observations)):\n",
        "            \n",
        "            # sample the latents z one time step at a time\n",
        "            # we wrap this loop in pyro.markov so that TraceEnum_ELBO can use multiple samples from the guide at each z\n",
        "            \n",
        "            for t in pyro.markov(range(1, T_max + 1)):\n",
        "                \n",
        "                z_loc, z_scale = self.proposer(z_prev, rnn_output[:, t - 1, :],a_prev,constants)\n",
        "\n",
        "                # if we are using normalizing flows, we apply the sequence of transformations\n",
        "                # parameterized by self.iafs to the base distribution defined in the previous line\n",
        "                # to yield a transformed distribution that we use for q(z_t|...)\n",
        "                \n",
        "                if len(self.iafs) > 0:\n",
        "                    z_dist = TransformedDistribution(dist.Normal(z_loc, z_scale), self.iafs)\n",
        "                    assert z_dist.event_shape == (self.z_q_0.size(0),)\n",
        "                    assert z_dist.batch_shape[-1:] == (len(observations),)\n",
        "                \n",
        "                else:\n",
        "                    z_dist = dist.Normal(z_loc, z_scale)\n",
        "                    assert z_dist.event_shape == ()\n",
        "                    # assert z_dist.batch_shape[-2:] == (len(observations), z_prev.size(0))\n",
        "\n",
        "                # sample z_t from the distribution z_dist\n",
        "               \n",
        "                with pyro.poutine.scale(scale=annealing_factor):\n",
        "                    if len(self.iafs) > 0:\n",
        "                        # in output of normalizing flow, all dimensions are correlated (event shape is not empty)\n",
        "                        z_t = pyro.sample(\"z_%d\" % t,\n",
        "                                          z_dist.mask(mask[:, t - 1]))\n",
        "                    \n",
        "                    else:\n",
        "                        # when no normalizing flow used, \".to_event(1)\" indicates latent dimensions are independent\n",
        "                        z_t = pyro.sample(\"z_%d\" % t,\n",
        "                                          z_dist.mask(mask[:, t - 1:t])\n",
        "                                          .to_event(1))\n",
        "                \n",
        "                \n",
        "                z_prev = z_t\n",
        "                u_prev=actions[:,t-1,:]\n",
        "\n",
        "    self.model=model\n",
        "    self.guide=guide\n",
        "\n",
        "\n",
        "def get_annhealing_factor(epoch):\n",
        "  if epoch<10:\n",
        "    return 0.25\n",
        "  else:\n",
        "     return min(1.0,0.25+0.05*epoch)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58WbigvyooUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def validate(trajectories,treatments,constants,mask,obs_lens):\n",
        "    # put the  into evaluation mode (i.e. turn off drop-out if applicable)\n",
        "    dmm.rnn.eval()\n",
        "    dmm.emitter.eval()\n",
        "    dmm.trans.eval()\n",
        "\n",
        "    val_nll = svi.evaluate_loss(dmm,trajectories,obs_lens,treatments,mask,constants,\n",
        "               binary=None,annealing_factor=annhealing_factor\n",
        "                               ) / np.sum(obs_lens)\n",
        "\n",
        "    dmm.rnn.train()\n",
        "    dmm.emitter.train()\n",
        "    dmm.trans.train()\n",
        "    return val_nll"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSQU06c5UNJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"### Defining Model and hyperparamters\"\"\"\n",
        "\n",
        "learning_rate=0.001\n",
        "beta1=0.96\n",
        "beta2=0.999\n",
        "clip_norm=5.0\n",
        "lr_decay=0.95\n",
        "weight_decay=0.0\n",
        "\n",
        "\n",
        "dmm=DMM(64,2,12,3,64)\n",
        "\n",
        "N_epochs=5000\n",
        "annhealing_factor=1.0\n",
        "\n",
        "adam_params = {\"lr\": learning_rate, \"betas\": (beta1, beta2),\n",
        "                   \"clip_norm\": clip_norm, \"lrd\": lr_decay,\n",
        "                   \"weight_decay\": weight_decay}\n",
        "optimizer = ClippedAdam(adam_params)\n",
        "\n",
        "# setup inference algorithm\n",
        "svi = SVI(dmm.model, dmm.guide, optimizer, Trace_ELBO(num_particles=5))\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qkob0brnXQjl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "106f817a-6c47-4f05-8776-6c4f016866da"
      },
      "source": [
        "# dmm.load_state_dict(torch.load('state_dict_val_best.pt', map_location=torch.device('cpu')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULhmaT2jvenQ",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wodx_pmMUd_8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "772f1ee2-7aa7-4c19-ea69-79e2974738df"
      },
      "source": [
        "times = [time.time()]\n",
        "for epoch in range(3,N_epochs):\n",
        "  val_nll=0\n",
        "  train_nll=0\n",
        "  val_steps=0\n",
        "  train_steps=0\n",
        "  dmm.train()\n",
        "  # for i,(batch,masks,actions,batch_lens) in enumerate(train_loader):\n",
        "  for i,(trajectories,treatments,constants,mask,obs_lens) in enumerate(train_loader):\n",
        "\n",
        "    # obs_lens=[24 for i in range(trajectories.shape[0])]\n",
        "\n",
        "    if min(obs_lens)<2:\n",
        "      continue\n",
        "\n",
        "\n",
        "    loss = svi.step(dmm,trajectories,obs_lens,treatments,mask,constants,\n",
        "               binary=None,annealing_factor=annhealing_factor)\n",
        "    \n",
        "    print(loss)\n",
        "    \n",
        "    batch_nll=svi.evaluate_loss(dmm,trajectories,obs_lens,treatments,mask,constants\n",
        "                                 ) / np.sum(obs_lens)\n",
        "\n",
        "    # if batch_nll<100:  \n",
        "    #    torch.save(dmm.state_dict(),'state_dict_49.pt')\n",
        "    \n",
        "    \n",
        "    train_nll+=batch_nll\n",
        "    train_steps+=1\n",
        "    print(batch_nll)\n",
        "    print('Epoch ',epoch,'Batch : ', train_steps, ' Training Loss :',train_nll/train_steps,end='')\n",
        "\n",
        "   \n",
        "    \n",
        "    \n",
        "     \n",
        "  val_nll=0\n",
        "  val_steps=0\n",
        "  for trajectories,treatments,constants,mask,obs_lens in valid_loader:\n",
        "      if min(obs_lens)==0:\n",
        "          continue\n",
        "      \n",
        "      val_nll+=validate(trajectories,treatments,constants,mask,obs_lens)\n",
        "      print('Validating : ',validate(trajectories,treatments,constants,mask,obs_lens))\n",
        "      val_steps+=1 \n",
        "\n",
        "      \n",
        "      print(val_nll/val_steps) \n",
        "  \n",
        "  if val_nll/val_steps<40:\n",
        "          torch.save(dmm.state_dict(),'state_dict_{}.pt'.format(val_nll/val_steps))\n",
        "       \n",
        "     \n",
        "\n",
        "\n",
        "  print('-'*125)\n",
        "  if val_nll/val_steps<30:  \n",
        "    torch.save(dmm.state_dict(),'state_dict_{}.pt'.format(val_nll/val_steps))\n",
        "  \n",
        "  # writer.add_scalar('train_nll', train_nll/train_steps, epoch)\n",
        "  print('Train nll {}'.format(train_nll/train_steps))\n",
        "  # writer.add_scalar('vall_nll',val_nll/val_steps, epoch)\n",
        "  print('Validation nll {}'.format(val_nll/val_steps))  \n",
        "  # annhealing_factor=get_annhealing_factor(epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "32243.592254638672\n",
            "23.134495449678802\n",
            "Epoch  3 Batch :  1  Training Loss : 23.13449544967880225045.700213623048\n",
            "17.349431426602344\n",
            "Epoch  3 Batch :  2  Training Loss : 20.24196343814057329357.507577514643\n",
            "20.921051217765044\n",
            "Epoch  3 Batch :  3  Training Loss : 20.46832603134872725996.42530517578\n",
            "18.76891124636628\n",
            "Epoch  3 Batch :  4  Training Loss : 20.04347233510311830471.713769531252\n",
            "21.13378731021394\n",
            "Epoch  3 Batch :  5  Training Loss : 20.2615353301252828738.426162719727\n",
            "19.910382543103452\n",
            "Epoch  3 Batch :  6  Training Loss : 20.2030098656216422772.161486816407\n",
            "16.101307038407327\n",
            "Epoch  3 Batch :  7  Training Loss : 19.6170523188767428211.18526306152\n",
            "20.707481268274854\n",
            "Epoch  3 Batch :  8  Training Loss : 19.75335593755150431017.821862792967\n",
            "21.328763615145228\n",
            "Epoch  3 Batch :  9  Training Loss : 19.92840123506191623323.899560546877\n",
            "16.799601426235466\n",
            "Epoch  3 Batch :  10  Training Loss : 19.6155212541792726727.874435424805\n",
            "18.586425507703083\n",
            "Epoch  3 Batch :  11  Training Loss : 19.5219670954087123216.680039978026\n",
            "16.07477432943604\n",
            "Epoch  3 Batch :  12  Training Loss : 19.23470103157765421954.25844421387\n",
            "15.192656790657438\n",
            "Epoch  3 Batch :  13  Training Loss : 18.92377455150686726906.18656616211\n",
            "19.340108676046174\n",
            "Epoch  3 Batch :  14  Training Loss : 18.95351270325967322731.336975097656\n",
            "15.440222909556311\n",
            "Epoch  3 Batch :  15  Training Loss : 18.7192933836794533724.583068847656\n",
            "23.71391774222065\n",
            "Epoch  3 Batch :  16  Training Loss : 19.03145740608827623966.79177246094\n",
            "17.33290620484104\n",
            "Epoch  3 Batch :  17  Training Loss : 18.9315426295443230336.074826049804\n",
            "21.30754080908446\n",
            "Epoch  3 Batch :  18  Training Loss : 19.06354252840766325342.822674560546\n",
            "17.52971242198336\n",
            "Epoch  3 Batch :  19  Training Loss : 18.98281462806953731318.32306213379\n",
            "21.863205856643354\n",
            "Epoch  3 Batch :  20  Training Loss : 19.1268341894982334263.92626037597\n",
            "23.67210086417598\n",
            "Epoch  3 Batch :  21  Training Loss : 19.34327545972097828209.8095703125\n",
            "19.62863493723849\n",
            "Epoch  3 Batch :  22  Training Loss : 19.35624634506268323388.99404296875\n",
            "16.357170466457024\n",
            "Epoch  3 Batch :  23  Training Loss : 19.22585174164504825869.777642822268\n",
            "18.376460022919606\n",
            "Epoch  3 Batch :  24  Training Loss : 19.1904604200314937601.029107666014\n",
            "25.63480200543848\n",
            "Epoch  3 Batch :  25  Training Loss : 19.44823408344776528219.17344970703\n",
            "21.400348676694595\n",
            "Epoch  3 Batch :  26  Training Loss : 19.5233154139572631713.089395141607\n",
            "21.85468316874567\n",
            "Epoch  3 Batch :  27  Training Loss : 19.6096623678383125089.79375\n",
            "18.119505283598265\n",
            "Epoch  3 Batch :  28  Training Loss : 19.55644247197259534768.82364501953\n",
            "24.85734912249284\n",
            "Epoch  3 Batch :  29  Training Loss : 19.7392323564732929416.322271728513\n",
            "21.13110225644699\n",
            "Epoch  3 Batch :  30  Training Loss : 19.7856280198057523490.65160675049\n",
            "17.29654442715959\n",
            "Epoch  3 Batch :  31  Training Loss : 19.7053350006881330025.01473388672\n",
            "21.30093397141849\n",
            "Epoch  3 Batch :  32  Training Loss : 19.75519746852345533725.48678283692\n",
            "23.69010158463173\n",
            "Epoch  3 Batch :  33  Training Loss : 19.87443698719340539046.04715270996\n",
            "28.04554352678571\n",
            "Epoch  3 Batch :  34  Training Loss : 20.1147636501225921693.533502197268\n",
            "15.588781906227373\n",
            "Epoch  3 Batch :  35  Training Loss : 19.985449886011329983.35655822754\n",
            "20.535051976386036\n",
            "Epoch  3 Batch :  36  Training Loss : 20.00071661074393223272.805146789553\n",
            "16.590836902212704\n",
            "Epoch  3 Batch :  37  Training Loss : 19.90855769970254522424.381298828128\n",
            "15.953404137010677\n",
            "Epoch  3 Batch :  38  Training Loss : 19.80447471121065635966.34341430664\n",
            "24.815577553485163\n",
            "Epoch  3 Batch :  39  Training Loss : 19.93296452767923225818.86691589355\n",
            "19.525535781722052\n",
            "Epoch  3 Batch :  40  Training Loss : 19.922778809030330604.47593078613\n",
            "21.14317439862543\n",
            "Epoch  3 Batch :  41  Training Loss : 19.9525445551179932914.723803710935\n",
            "24.018142993804666\n",
            "Epoch  3 Batch :  42  Training Loss : 20.0493445179438623723.87863006592\n",
            "16.72963856761566\n",
            "Epoch  3 Batch :  43  Training Loss : 19.9721420539827428595.36110534668\n",
            "21.008455076698322\n",
            "Epoch  3 Batch :  44  Training Loss : 19.9956946226808220259.875827026368\n",
            "14.847624863038714\n",
            "Epoch  3 Batch :  45  Training Loss : 19.8812930724665533200.906478881836\n",
            "22.47729258102633\n",
            "Epoch  3 Batch :  46  Training Loss : 19.93772784439176234568.82940673828\n",
            "24.29988174491941\n",
            "Epoch  3 Batch :  47  Training Loss : 20.030539629509374110021.52389526367\n",
            "76.85922743055556\n",
            "Epoch  3 Batch :  48  Training Loss : 21.21447062536450223106.16620178223\n",
            "16.182056165839832\n",
            "Epoch  3 Batch :  49  Training Loss : 21.1117682894558433394.680676269534\n",
            "23.742840848688875\n",
            "Epoch  3 Batch :  50  Training Loss : 21.16438974064049824365.925250244138\n",
            "17.629830869175628\n",
            "Epoch  3 Batch :  51  Training Loss : 21.0950846647294234812.226022338866\n",
            "25.28224904960174\n",
            "Epoch  3 Batch :  52  Training Loss : 21.17560705674619522566.6405960083\n",
            "16.00377383474576\n",
            "Epoch  3 Batch :  53  Training Loss : 21.07802529784052625724.702313232425\n",
            "18.07801214060309\n",
            "Epoch  3 Batch :  54  Training Loss : 21.02246949863242531403.506927490234\n",
            "21.679445061517427\n",
            "Epoch  3 Batch :  55  Training Loss : 21.034414508866740367.10701293945\n",
            "27.739499049100964\n",
            "Epoch  3 Batch :  56  Training Loss : 21.1541481613708824572.4918182373\n",
            "17.575600704350926\n",
            "Epoch  3 Batch :  57  Training Loss : 21.091366627037231655.1942199707\n",
            "21.750632287379972\n",
            "Epoch  3 Batch :  58  Training Loss : 21.10273327635345526227.247399902342\n",
            "17.902230143004825\n",
            "Epoch  3 Batch :  59  Training Loss : 21.04848746053398435155.98717651367\n",
            "24.951906404812455\n",
            "Epoch  3 Batch :  60  Training Loss : 21.11354444293862531442.81723480225\n",
            "21.993933242626404\n",
            "Epoch  3 Batch :  61  Training Loss : 21.12797704621219529886.006704711912\n",
            "20.43807198660714\n",
            "Epoch  3 Batch :  62  Training Loss : 21.11684954525082222541.7176361084\n",
            "16.085598550853486\n",
            "Epoch  3 Batch :  63  Training Loss : 21.03698841835562734215.71213684082\n",
            "23.890332576815645\n",
            "Epoch  3 Batch :  64  Training Loss : 21.08157192083156424694.29362487793\n",
            "18.154509420955883\n",
            "Epoch  3 Batch :  65  Training Loss : 21.03654019006424725115.06804199219\n",
            "18.303601771511207\n",
            "Epoch  3 Batch :  66  Training Loss : 20.99513203220738328246.091625976565\n",
            "20.25716408501441\n",
            "Epoch  3 Batch :  67  Training Loss : 20.98411758523435527646.811422729494\n",
            "19.629129543834637\n",
            "Epoch  3 Batch :  68  Training Loss : 20.96419129050788826951.294871520997\n",
            "20.0013450684678\n",
            "Epoch  3 Batch :  69  Training Loss : 20.95023699743484326725.801757812496\n",
            "18.516095580634047\n",
            "Epoch  3 Batch :  70  Training Loss : 20.91546354862340426349.896484375\n",
            "18.15290988869863\n",
            "Epoch  3 Batch :  71  Training Loss : 20.8765543421455927780.071652221683\n",
            "19.78238242354196\n",
            "Epoch  3 Batch :  72  Training Loss : 20.8613575099427630048.55066833496\n",
            "20.715829227178425\n",
            "Epoch  3 Batch :  73  Training Loss : 20.859363971822725207.166421508788\n",
            "17.77765436746988\n",
            "Epoch  3 Batch :  74  Training Loss : 20.81771924743955525831.476846313475\n",
            "18.209221792420028\n",
            "Epoch  3 Batch :  75  Training Loss : 20.78293928137262728107.84042663574\n",
            "19.31933970385675\n",
            "Epoch  3 Batch :  76  Training Loss : 20.76368139219478723557.44877319336\n",
            "17.218665883112095\n",
            "Epoch  3 Batch :  77  Training Loss : 20.7176422297391722850.860681152342\n",
            "15.867183107870696\n",
            "Epoch  3 Batch :  78  Training Loss : 20.65545685638188248469.57947387696\n",
            "35.41035975940666\n",
            "Epoch  3 Batch :  79  Training Loss : 20.8422277792049823849.182234191896\n",
            "16.40331237077877\n",
            "Epoch  3 Batch :  80  Training Loss : 20.78674133659965421846.609352111816\n",
            "15.646534697508898\n",
            "Epoch  3 Batch :  81  Training Loss : 20.7232819953763128759.045223999023\n",
            "20.466654554263567\n",
            "Epoch  3 Batch :  82  Training Loss : 20.72015239243591435602.417004394534\n",
            "24.26533236500342\n",
            "Epoch  3 Batch :  83  Training Loss : 20.76286540415359427103.558895874023\n",
            "19.29085926291248\n",
            "Epoch  3 Batch :  84  Training Loss : 20.7453415215197721948.423452758787\n",
            "15.856925798258345\n",
            "Epoch  3 Batch :  85  Training Loss : 20.6878307483049325621.136865234374\n",
            "18.34686038669065\n",
            "Epoch  3 Batch :  86  Training Loss : 20.66061016270476422055.178530883793\n",
            "15.51566313633194\n",
            "Epoch  3 Batch :  87  Training Loss : 20.6014728405625526295.609115600586\n",
            "18.723303031650072\n",
            "Epoch  3 Batch :  88  Training Loss : 20.5801300018249125197.74640960693\n",
            "18.014371197208305\n",
            "Epoch  3 Batch :  89  Training Loss : 20.5513012512112427514.2079284668\n",
            "19.290518143754362\n",
            "Epoch  3 Batch :  90  Training Loss : 20.53729255001727321230.04805908203\n",
            "14.54138735314444\n",
            "Epoch  3 Batch :  91  Training Loss : 20.4714034819197724324.34813232422\n",
            "17.62093456466763\n",
            "Epoch  3 Batch :  92  Training Loss : 20.4404201241235527591.587774658205\n",
            "20.401997817607725\n",
            "Epoch  3 Batch :  93  Training Loss : 20.44000698104273224798.570095825195\n",
            "17.823988381410253\n",
            "Epoch  3 Batch :  94  Training Loss : 20.4121769959402630845.723559570317\n",
            "22.21542305956679\n",
            "Epoch  3 Batch :  95  Training Loss : 20.4311585334521224438.882708740235\n",
            "16.912271576870285\n",
            "Epoch  3 Batch :  96  Training Loss : 20.39450346098772539311.55817108154\n",
            "27.964095697592068\n",
            "Epoch  3 Batch :  97  Training Loss : 20.4725404943547825549.76310424805\n",
            "18.46689572819034\n",
            "Epoch  3 Batch :  98  Training Loss : 20.45207473143473528535.080746459964\n",
            "19.89206028183716\n",
            "Epoch  3 Batch :  99  Training Loss : 20.44641801982263736854.306604003905\n",
            "26.340725189121034\n",
            "Epoch  3 Batch :  100  Training Loss : 20.50536109151562427236.208810424803\n",
            "19.112056853991596\n",
            "Epoch  3 Batch :  101  Training Loss : 20.4915660000549926371.01676635742\n",
            "17.848778235394022\n",
            "Epoch  3 Batch :  102  Training Loss : 20.46565631608772832565.036697387695\n",
            "21.981057854729734\n",
            "Epoch  3 Batch :  103  Training Loss : 20.48036895238522725612.49883117676\n",
            "18.06269944464034\n",
            "Epoch  3 Batch :  104  Training Loss : 20.4571221301953726297.77463684082\n",
            "18.80983829294369\n",
            "Epoch  3 Batch :  105  Training Loss : 20.44143371269773724343.980041503903\n",
            "17.049282715185445\n",
            "Epoch  3 Batch :  106  Training Loss : 20.40943228819290322190.217265319825\n",
            "15.50925127014716\n",
            "Epoch  3 Batch :  107  Training Loss : 20.36363620391210529181.05877685547\n",
            "21.070535232181427\n",
            "Epoch  3 Batch :  108  Training Loss : 20.3701815652849730132.97630462647\n",
            "21.839628213251267\n",
            "Epoch  3 Batch :  109  Training Loss : 20.38366272719291728661.015509033205\n",
            "20.29309245249824\n",
            "Epoch  3 Batch :  110  Training Loss : 20.38283936105932524447.526629638673\n",
            "17.287371908127206\n",
            "Epoch  3 Batch :  111  Training Loss : 20.35495226688876633846.60728759765\n",
            "24.038542774822695\n",
            "Epoch  3 Batch :  112  Training Loss : 20.38784146785246424428.19745788574\n",
            "17.76865284381858\n",
            "Epoch  3 Batch :  113  Training Loss : 20.36466280746278232063.508663940433\n",
            "22.48195883075843\n",
            "Epoch  3 Batch :  114  Training Loss : 20.38323557959695326796.412606811522\n",
            "19.102829512893983\n",
            "Epoch  3 Batch :  115  Training Loss : 20.37210161379953634766.48732299805\n",
            "23.899020362765228\n",
            "Epoch  3 Batch :  116  Training Loss : 20.4025060857733820576.017922973635\n",
            "14.78921464108031\n",
            "Epoch  3 Batch :  117  Training Loss : 20.35452923581873824595.25320129395\n",
            "17.49000376506024\n",
            "Epoch  3 Batch :  118  Training Loss : 20.3302535962360429556.330197143554\n",
            "20.817497799295776\n",
            "Epoch  3 Batch :  119  Training Loss : 20.33434808533738521643.952333068846\n",
            "15.222253887141857\n",
            "Epoch  3 Batch :  120  Training Loss : 20.29174730035242325741.021176147464\n",
            "18.442695284538296\n",
            "Epoch  3 Batch :  121  Training Loss : 20.27646587873412527239.46912536621\n",
            "19.46132588646132\n",
            "Epoch  3 Batch :  122  Training Loss : 20.2697844033876381777.19395751953\n",
            "58.73655377594846\n",
            "Epoch  3 Batch :  123  Training Loss : 20.5825223657661726019.13477783203\n",
            "17.912127264183184\n",
            "Epoch  3 Batch :  124  Training Loss : 20.56098692139856630255.036138916013\n",
            "21.261648224282716\n",
            "Epoch  3 Batch :  125  Training Loss : 20.5665922118216423069.612623596193\n",
            "16.12187935236769\n",
            "Epoch  3 Batch :  126  Training Loss : 20.53131671293708425370.321640014645\n",
            "18.23703001615219\n",
            "Epoch  3 Batch :  127  Training Loss : 20.51325146335610225213.78408203125\n",
            "17.79787252824859\n",
            "Epoch  3 Batch :  128  Training Loss : 20.49203756542557624983.26847839356\n",
            "17.61256759751773\n",
            "Epoch  3 Batch :  129  Training Loss : 20.46971609280613423995.547158813475\n",
            "16.610138960926697\n",
            "Epoch  3 Batch :  130  Training Loss : 20.44002703794552338742.07881469726\n",
            "27.27046115221987\n",
            "Epoch  3 Batch :  131  Training Loss : 20.492167756375130954.409564208985\n",
            "21.920380434782608\n",
            "Epoch  3 Batch :  132  Training Loss : 20.5029875493933428396.95867919922\n",
            "20.661101255458515\n",
            "Epoch  3 Batch :  133  Training Loss : 20.5041763742509724389.00163269043\n",
            "17.307913270729976\n",
            "Epoch  3 Batch :  134  Training Loss : 20.480323664523231814.90255126953\n",
            "22.163252880586594\n",
            "Epoch  3 Batch :  135  Training Loss : 20.49278980686441221241.76215362549\n",
            "14.97604240282686\n",
            "Epoch  3 Batch :  136  Training Loss : 20.45222548771707624118.92128601074\n",
            "17.25432562229437\n",
            "Epoch  3 Batch :  137  Training Loss : 20.4288831529329723342.059188842773\n",
            "16.64755551575931\n",
            "Epoch  3 Batch :  138  Training Loss : 20.40148222802591619476.177445983885\n",
            "14.19867159090909\n",
            "Epoch  3 Batch :  139  Training Loss : 20.35685769106824221133.50164337158\n",
            "14.808585640138409\n",
            "Epoch  3 Batch :  140  Training Loss : 20.31722717641874425953.077114868167\n",
            "18.71437365687679\n",
            "Epoch  3 Batch :  141  Training Loss : 20.30585942096129445.896841430662\n",
            "20.356861835290058\n",
            "Epoch  3 Batch :  142  Training Loss : 20.30621859289289726425.033929443358\n",
            "18.607155187677055\n",
            "Epoch  3 Batch :  143  Training Loss : 20.2943370306186626921.403311157228\n",
            "19.23364724500713\n",
            "Epoch  3 Batch :  144  Training Loss : 20.2869711293296928630.241101074218\n",
            "20.305017262747874\n",
            "Epoch  3 Batch :  145  Training Loss : 20.2870955854222339507.2169921875\n",
            "27.014288778311602\n",
            "Epoch  3 Batch :  146  Training Loss : 20.3331722511269539987.142486572266\n",
            "29.39895107300073\n",
            "Epoch  3 Batch :  147  Training Loss : 20.394844215901638111.834677124025\n",
            "27.794171989051094\n",
            "Epoch  3 Batch :  148  Training Loss : 20.44483967382828740203.52825622559\n",
            "28.571021217221833\n",
            "Epoch  3 Batch :  149  Training Loss : 20.49937780499222010.816137695314\n",
            "15.286763361768804\n",
            "Epoch  3 Batch :  150  Training Loss : 20.4646270420371827683.362652587894\n",
            "18.990873117727585\n",
            "Epoch  3 Batch :  151  Training Loss : 20.45486708227353731605.07894592285\n",
            "22.040840063202246\n",
            "Epoch  3 Batch :  152  Training Loss : 20.46530111504280627342.056408691406\n",
            "19.60554150250716\n",
            "Epoch  3 Batch :  153  Training Loss : 20.45968177117002522593.853713989258\n",
            "15.960066579985654\n",
            "Epoch  3 Batch :  154  Training Loss : 20.43046349070778721333.271984863277\n",
            "15.176277290622764\n",
            "Epoch  3 Batch :  155  Training Loss : 20.3965655152233723925.732272338864\n",
            "18.1044929245283\n",
            "Epoch  3 Batch :  156  Training Loss : 20.3818727422060925633.03083496094\n",
            "18.302040586753062\n",
            "Epoch  3 Batch :  157  Training Loss : 20.3686254036363321730.990553283693\n",
            "15.506681303495007\n",
            "Epoch  3 Batch :  158  Training Loss : 20.33785360553416825577.189535522462\n",
            "18.288021574733094\n",
            "Epoch  3 Batch :  159  Training Loss : 20.32496158018321730204.575244140622\n",
            "21.352425324106516\n",
            "Epoch  3 Batch :  160  Training Loss : 20.3313832285827425130.731369018555\n",
            "17.132952296971784\n",
            "Epoch  3 Batch :  161  Training Loss : 20.31151719795161626438.50920104981\n",
            "19.228552263708515\n",
            "Epoch  3 Batch :  162  Training Loss : 20.3048322292217230204.694912719722\n",
            "21.05847435450105\n",
            "Epoch  3 Batch :  163  Training Loss : 20.3094558005424525765.26853637695\n",
            "19.039023089762612\n",
            "Epoch  3 Batch :  164  Training Loss : 20.3017092596230625901.19541168213\n",
            "17.66176227072011\n",
            "Epoch  3 Batch :  165  Training Loss : 20.28570958090243821849.859741210938\n",
            "16.052741228070175\n",
            "Epoch  3 Batch :  166  Training Loss : 20.26020977154802731242.195379638673\n",
            "22.72586564781022\n",
            "Epoch  3 Batch :  167  Training Loss : 20.2749741779927127299.904031372072\n",
            "19.495675203467798\n",
            "Epoch  3 Batch :  168  Training Loss : 20.27033549362053625745.187756347656\n",
            "18.174652039007093\n",
            "Epoch  3 Batch :  169  Training Loss : 20.25793499980625625705.587301635744\n",
            "17.816928714335422\n",
            "Epoch  3 Batch :  170  Training Loss : 20.24357613930348522660.15464477539\n",
            "17.384570913461538\n",
            "Epoch  3 Batch :  171  Training Loss : 20.22685681049739523750.3493927002\n",
            "16.81983552631579\n",
            "Epoch  3 Batch :  172  Training Loss : 20.2070485472172726083.225946044924\n",
            "18.32221548836389\n",
            "Epoch  3 Batch :  173  Training Loss : 20.19615355843776832279.000413513182\n",
            "23.90632191907944\n",
            "Epoch  3 Batch :  174  Training Loss : 20.21747636510812334821.84638671875\n",
            "23.84933433219178\n",
            "Epoch  3 Batch :  175  Training Loss : 20.23822983920574421438.81416625977\n",
            "15.239147993160548\n",
            "Epoch  3 Batch :  176  Training Loss : 20.20982596508048724781.90093231201\n",
            "17.96558627915452\n",
            "Epoch  3 Batch :  177  Training Loss : 20.19714664482101831398.22610168457\n",
            "23.20195513409258\n",
            "Epoch  3 Batch :  178  Training Loss : 20.2140275913899629235.594653320313\n",
            "20.665133081517922\n",
            "Epoch  3 Batch :  179  Training Loss : 20.2165477337929131003.34889526367\n",
            "20.96104910714286\n",
            "Epoch  3 Batch :  180  Training Loss : 20.22068385253374231935.978045654298\n",
            "21.51504301619433\n",
            "Epoch  3 Batch :  181  Training Loss : 20.2278350081340826789.73254699707\n",
            "18.728432682200424\n",
            "Epoch  3 Batch :  182  Training Loss : 20.2195965338157631655.89357910156\n",
            "21.614534859035324\n",
            "Epoch  3 Batch :  183  Training Loss : 20.22721914761477525937.02625427246\n",
            "19.23875512295082\n",
            "Epoch  3 Batch :  184  Training Loss : 20.2218470605242138516.58530883789\n",
            "26.288453969883637\n",
            "Epoch  3 Batch :  185  Training Loss : 20.2546395303045325853.52517089844\n",
            "18.74178228268678\n",
            "Epoch  3 Batch :  186  Training Loss : 20.24650588918830620760.287228393554\n",
            "14.731207890070925\n",
            "Epoch  3 Batch :  187  Training Loss : 20.21701231700051329997.735668945315\n",
            "21.711291681851314\n",
            "Epoch  3 Batch :  188  Training Loss : 20.22496061149440325709.980831909183\n",
            "18.534370490620493\n",
            "Epoch  3 Batch :  189  Training Loss : 20.21601569021993726285.69524230957\n",
            "18.557226837201124\n",
            "Epoch  3 Batch :  190  Training Loss : 20.2072852225724726052.366061401364\n",
            "18.43792359620887\n",
            "Epoch  3 Batch :  191  Training Loss : 20.1980215491360125840.117874145508\n",
            "17.96752627692842\n",
            "Epoch  3 Batch :  192  Training Loss : 20.18640438625992725323.43453063965\n",
            "18.382542675216765\n",
            "Epoch  3 Batch :  193  Training Loss : 20.17705795252395321282.1916015625\n",
            "15.142484065155807\n",
            "Epoch  3 Batch :  194  Training Loss : 20.15110654073339424435.597056579587\n",
            "17.523373101952277\n",
            "Epoch  3 Batch :  195  Training Loss : 20.1376309846370829887.984729003903\n",
            "21.19934268767705\n",
            "Epoch  3 Batch :  196  Training Loss : 20.14304788108116325002.97649383545\n",
            "17.289219944405836\n",
            "Epoch  3 Batch :  197  Training Loss : 20.12856144485438429089.22627868652\n",
            "20.460907579787232\n",
            "Epoch  3 Batch :  198  Training Loss : 20.13023996068737624085.104977416995\n",
            "16.960546059498956\n",
            "Epoch  3 Batch :  199  Training Loss : 20.11431185063115441743.65910034179\n",
            "28.58062650240384\n",
            "Epoch  3 Batch :  200  Training Loss : 20.15664342389001721626.67110900879\n",
            "15.274774256373936\n",
            "Epoch  3 Batch :  201  Training Loss : 20.13235551758396726233.398880004883\n",
            "17.9130714649898\n",
            "Epoch  3 Batch :  202  Training Loss : 20.12136896286815724579.177432250974\n",
            "16.816501550654724\n",
            "Epoch  3 Batch :  203  Training Loss : 20.1050888278326228486.046221923825\n",
            "20.81973980880231\n",
            "Epoch  3 Batch :  204  Training Loss : 20.10859201891580741194.16768188476\n",
            "30.65824341367713\n",
            "Epoch  3 Batch :  205  Training Loss : 20.16005373303659528015.256567382814\n",
            "20.188878210926195\n",
            "Epoch  3 Batch :  206  Training Loss : 20.16019365768654537116.86917114258\n",
            "24.955343220338982\n",
            "Epoch  3 Batch :  207  Training Loss : 20.18335863141916827964.22882080078\n",
            "20.19625813449024\n",
            "Epoch  3 Batch :  208  Training Loss : 20.18342064826085629846.16800384521\n",
            "21.78577041184971\n",
            "Epoch  3 Batch :  209  Training Loss : 20.19108739354118527263.89837799072\n",
            "20.025620624539425\n",
            "Epoch  3 Batch :  210  Training Loss : 20.1902994565459429103.65563354492\n",
            "20.508282134016973\n",
            "Epoch  3 Batch :  211  Training Loss : 20.19180648345338424585.448402404785\n",
            "17.698304211469534\n",
            "Epoch  3 Batch :  212  Training Loss : 20.1800446802836522073.729566955568\n",
            "15.426778762975779\n",
            "Epoch  3 Batch :  213  Training Loss : 20.15772887785497523306.210214233397\n",
            "16.961028486700215\n",
            "Epoch  3 Batch :  214  Training Loss : 20.1427910255598627041.594305419923\n",
            "19.391883555475502\n",
            "Epoch  3 Batch :  215  Training Loss : 20.13929843267574831412.728186035158\n",
            "21.47689426369863\n",
            "Epoch  3 Batch :  216  Training Loss : 20.1454910059675226202.2616394043\n",
            "18.302921907756815\n",
            "Epoch  3 Batch :  217  Training Loss : 20.1369999041324528091.048641967773\n",
            "19.31847255677908\n",
            "Epoch  3 Batch :  218  Training Loss : 20.1332451915299136615.16967163086\n",
            "25.6899090829234\n",
            "Epoch  3 Batch :  219  Training Loss : 20.1586180860111626664.91741027832\n",
            "18.808540209790213\n",
            "Epoch  3 Batch :  220  Training Loss : 20.15248136839197529792.59217224121\n",
            "21.19934951241135\n",
            "Epoch  3 Batch :  221  Training Loss : 20.15721832831966524861.194744873046\n",
            "17.47281556587246\n",
            "Epoch  3 Batch :  222  Training Loss : 20.14512642398431623575.19072570801\n",
            "17.386966068786982\n",
            "Epoch  3 Batch :  223  Training Loss : 20.1327579918982324748.914154052734\n",
            "17.233590476606143\n",
            "Epoch  3 Batch :  224  Training Loss : 20.1198152797763926641.923278808594\n",
            "18.831300509487\n",
            "Epoch  3 Batch :  225  Training Loss : 20.11408854746399333601.03739624024\n",
            "23.40998033216783\n",
            "Epoch  3 Batch :  226  Training Loss : 20.12867213943170729923.091094970703\n",
            "20.801619730476848\n",
            "Epoch  3 Batch :  227  Training Loss : 20.13163666626450525481.120758056644\n",
            "17.685577090592332\n",
            "Epoch  3 Batch :  228  Training Loss : 20.12090833479225828062.42141113281\n",
            "19.962080008865247\n",
            "Epoch  3 Batch :  229  Training Loss : 20.12021476131659624251.971826171877\n",
            "17.410776785714287\n",
            "Epoch  3 Batch :  230  Training Loss : 20.10843459620527823742.481979370117\n",
            "16.663026685393255\n",
            "Epoch  3 Batch :  231  Training Loss : 20.0935194104441925396.909677124026\n",
            "18.295496060171917\n",
            "Epoch  3 Batch :  232  Training Loss : 20.08576930979646525006.64418640137\n",
            "17.48100944716585\n",
            "Epoch  3 Batch :  233  Training Loss : 20.0745900829182224797.39606933594\n",
            "17.39061951369382\n",
            "Epoch  3 Batch :  234  Training Loss : 20.0631201232206824272.12446136475\n",
            "16.71340449096595\n",
            "Epoch  3 Batch :  235  Training Loss : 20.0488660141472641139.45418701172\n",
            "29.407288694721828\n",
            "Epoch  3 Batch :  236  Training Loss : 20.08852034753952530019.98544006348\n",
            "21.603878348214284\n",
            "Epoch  3 Batch :  237  Training Loss : 20.09491426315418727482.6420501709\n",
            "18.978692694024726\n",
            "Epoch  3 Batch :  238  Training Loss : 20.09022425656120721488.999105834962\n",
            "15.3565625\n",
            "Epoch  3 Batch :  239  Training Loss : 20.07041814042496633953.5139831543\n",
            "24.495506676962208\n",
            "Epoch  3 Batch :  240  Training Loss : 20.0888560093272130716.829174804683\n",
            "21.707194355980185\n",
            "Epoch  3 Batch :  241  Training Loss : 20.09557110620128727281.8540435791\n",
            "19.221980042016806\n",
            "Epoch  3 Batch :  242  Training Loss : 20.09196122577077421480.21026382446\n",
            "16.18131721698113\n",
            "Epoch  3 Batch :  243  Training Loss : 20.0758680405494231066.93803100586\n",
            "21.20142570109439\n",
            "Epoch  3 Batch :  244  Training Loss : 20.0804809817811624044.314224243164\n",
            "16.720840881642513\n",
            "Epoch  3 Batch :  245  Training Loss : 20.066768165045922589.072326660156\n",
            "15.35890466101695\n",
            "Epoch  3 Batch :  246  Training Loss : 20.04763050852545427202.991009521484\n",
            "19.8607391782832\n",
            "Epoch  3 Batch :  247  Training Loss : 20.04687386346374824684.81870727539\n",
            "16.98770797413793\n",
            "Epoch  3 Batch :  248  Training Loss : 20.0345385171358224381.003091430663\n",
            "16.484183282208587\n",
            "Epoch  3 Batch :  249  Training Loss : 20.02028006237707622861.041624450685\n",
            "16.697744821947673\n",
            "Epoch  3 Batch :  250  Training Loss : 20.0069899214153627723.38613891602\n",
            "18.95149676724138\n",
            "Epoch  3 Batch :  251  Training Loss : 20.002784769406723133.815419006347\n",
            "16.948722836011687\n",
            "Epoch  3 Batch :  252  Training Loss : 19.99066547602020835255.06735839843\n",
            "24.83292269040903\n",
            "Epoch  3 Batch :  253  Training Loss : 20.00980483259881728894.873834228514\n",
            "20.788005714804065\n",
            "Epoch  3 Batch :  254  Training Loss : 20.01286861559962624959.01613769531\n",
            "18.61198936370482\n",
            "Epoch  3 Batch :  255  Training Loss : 20.0073749714745526863.239335632323\n",
            "19.17334263392857\n",
            "Epoch  3 Batch :  256  Training Loss : 20.00411703265601224296.088552856447\n",
            "17.84290620451237\n",
            "Epoch  3 Batch :  257  Training Loss : 19.9957076520017622255.095780944823\n",
            "15.630312718990892\n",
            "Epoch  3 Batch :  258  Training Loss : 19.9787875166024937591.76087646485\n",
            "27.849048158771282\n",
            "Epoch  3 Batch :  259  Training Loss : 20.00917462332901325881.30407104492\n",
            "17.95776056094183\n",
            "Epoch  3 Batch :  260  Training Loss : 20.0012845692429128098.538336181642\n",
            "19.695468749999996\n",
            "Epoch  3 Batch :  261  Training Loss : 20.00011286112320630381.778387451173\n",
            "21.653907362891736\n",
            "Epoch  3 Batch :  262  Training Loss : 20.00642505387804733406.17028198242\n",
            "24.144638502528903\n",
            "Epoch  3 Batch :  263  Training Loss : 20.02215970577405833217.499114990234\n",
            "22.67899329584775\n",
            "Epoch  3 Batch :  264  Training Loss : 20.03222346937282330931.86323699951\n",
            "22.980549648668635\n",
            "Epoch  3 Batch :  265  Training Loss : 20.0433492285399826948.757751464844\n",
            "19.70294785610465\n",
            "Epoch  3 Batch :  266  Training Loss : 20.04206952413232722404.876770019528\n",
            "15.724178784162579\n",
            "Epoch  3 Batch :  267  Training Loss : 20.02589764870172635871.403515624996\n",
            "25.81437094740634\n",
            "Epoch  3 Batch :  268  Training Loss : 20.04749642966704228009.97342529297\n",
            "19.93128026362984\n",
            "Epoch  3 Batch :  269  Training Loss : 20.04706439931002430832.78807067871\n",
            "22.40791228229318\n",
            "Epoch  3 Batch :  270  Training Loss : 20.0558082803581123366.11096496582\n",
            "16.60466208451202\n",
            "Epoch  3 Batch :  271  Training Loss : 20.0430734235468729334.293856811524\n",
            "20.851246675531915\n",
            "Epoch  3 Batch :  272  Training Loss : 20.0460446487379923975.61176300049\n",
            "17.075304470021415\n",
            "Epoch  3 Batch :  273  Training Loss : 20.0351628165815227010.295236206057\n",
            "18.928743239706908\n",
            "Epoch  3 Batch :  274  Training Loss : 20.03112478892869320464.293844604494\n",
            "14.37470439189189\n",
            "Epoch  3 Batch :  275  Training Loss : 20.01055598748492426332.72585754394\n",
            "17.81442569870484\n",
            "Epoch  3 Batch :  276  Training Loss : 20.00259899368499829367.749166870115\n",
            "20.63879004726891\n",
            "Epoch  3 Batch :  277  Training Loss : 20.00489571228999427678.56565246582\n",
            "19.945799731182795\n",
            "Epoch  3 Batch :  278  Training Loss : 20.00468313681838623204.111730957033\n",
            "16.554621604717653\n",
            "Epoch  3 Batch :  279  Training Loss : 19.99231732487537325998.6922454834\n",
            "18.03428789198606\n",
            "Epoch  3 Batch :  280  Training Loss : 19.98532436261505721903.613191223143\n",
            "15.599351184116808\n",
            "Epoch  3 Batch :  281  Training Loss : 19.96971591713997227359.84853820801\n",
            "19.94825067934783\n",
            "Epoch  3 Batch :  282  Training Loss : 19.9696397992754638594.48901672363\n",
            "27.205434734901687\n",
            "Epoch  3 Batch :  283  Training Loss : 19.99520797926000550019.14692993164\n",
            "35.12470580543933\n",
            "Epoch  3 Batch :  284  Training Loss : 20.0484808589296530665.20528564453\n",
            "21.59846137152778\n",
            "Epoch  3 Batch :  285  Training Loss : 20.0539193870440335755.68835449219\n",
            "26.355579044117647\n",
            "Epoch  3 Batch :  286  Training Loss : 20.07595316206876426325.88393249512\n",
            "18.633322636331442\n",
            "Epoch  3 Batch :  287  Training Loss : 20.07092657487107336172.89433898926\n",
            "25.362840629395215\n",
            "Epoch  3 Batch :  288  Training Loss : 20.08930127644928434387.78918457031\n",
            "24.375420158701857\n",
            "Epoch  3 Batch :  289  Training Loss : 20.1041321376335529180.713760375977\n",
            "20.628691058718864\n",
            "Epoch  3 Batch :  290  Training Loss : 20.10594096149935724859.54396972656\n",
            "17.765425106609808\n",
            "Epoch  3 Batch :  291  Training Loss : 20.0978979516887421740.90712280273\n",
            "15.420611490077958\n",
            "Epoch  3 Batch :  292  Training Loss : 20.08187984736815727862.150473022462\n",
            "19.335277151283833\n",
            "Epoch  3 Batch :  293  Training Loss : 20.0793317152996123379.477822875975\n",
            "16.937995317002883\n",
            "Epoch  3 Batch :  294  Training Loss : 20.06864689761832622715.617718505862\n",
            "16.460442127965493\n",
            "Epoch  3 Batch :  295  Training Loss : 20.05641569500933527436.555856323244\n",
            "19.07624496320953\n",
            "Epoch  3 Batch :  296  Training Loss : 20.05310430740190234488.57214050293\n",
            "25.072567061054915\n",
            "Epoch  3 Batch :  297  Training Loss : 20.07000485539399627926.10083618164\n",
            "19.3531368780235\n",
            "Epoch  3 Batch :  298  Training Loss : 20.06759925815449820787.19744873047\n",
            "14.801340076133144\n",
            "Epoch  3 Batch :  299  Training Loss : 20.04998635119121535539.21332397461\n",
            "25.53355821299639\n",
            "Epoch  3 Batch :  300  Training Loss : 20.068264924063926448.5276550293\n",
            "19.32526707450694\n",
            "Epoch  3 Batch :  301  Training Loss : 20.0657964926700229016.952487182618\n",
            "20.98401685647144\n",
            "Epoch  3 Batch :  302  Training Loss : 20.06883695745082432720.811001586917\n",
            "22.806297709923665\n",
            "Epoch  3 Batch :  303  Training Loss : 20.07787148138637426720.41464538574\n",
            "19.17632306654676\n",
            "Epoch  3 Batch :  304  Training Loss : 20.0749058616007223810.374105834962\n",
            "16.391693513745704\n",
            "Epoch  3 Batch :  305  Training Loss : 20.0628297555421829340.013639831544\n",
            "20.77927969858156\n",
            "Epoch  3 Batch :  306  Training Loss : 20.06517109522531328753.33497467041\n",
            "20.52482193732194\n",
            "Epoch  3 Batch :  307  Training Loss : 20.06666832923865625811.056910705563\n",
            "18.480576220598717\n",
            "Epoch  3 Batch :  308  Training Loss : 20.0615186795352825107.509190368648\n",
            "17.6724091029724\n",
            "Epoch  3 Batch :  309  Training Loss : 20.05378693333281235534.16293945313\n",
            "25.41326061741613\n",
            "Epoch  3 Batch :  310  Training Loss : 20.07107555812017824614.60550842285\n",
            "17.570911235754988\n",
            "Epoch  3 Batch :  311  Training Loss : 20.06303644454344220775.694610595707\n",
            "15.126888873726346\n",
            "Epoch  3 Batch :  312  Training Loss : 20.0472154587395428306.338162231445\n",
            "20.325543673616107\n",
            "Epoch  3 Batch :  313  Training Loss : 20.0481046862631127395.96219177246\n",
            "18.91530426490985\n",
            "Epoch  3 Batch :  314  Training Loss : 20.04449704160911626376.32882080078\n",
            "19.45109157827359\n",
            "Epoch  3 Batch :  315  Training Loss : 20.04261321474138725679.74470825195\n",
            "18.637780390738058\n",
            "Epoch  3 Batch :  316  Training Loss : 20.03816754124770624422.915876770017\n",
            "17.750296433743667\n",
            "Epoch  3 Batch :  317  Training Loss : 20.03095028223349835691.5535736084\n",
            "25.78152247648336\n",
            "Epoch  3 Batch :  318  Training Loss : 20.04903384259277422645.78013000488\n",
            "15.985860710470085\n",
            "Epoch  3 Batch :  319  Training Loss : 20.03629662274285627881.700564575192\n",
            "20.603201844262294\n",
            "Epoch  3 Batch :  320  Training Loss : 20.03806820156010526800.30340576172\n",
            "18.50344723183391\n",
            "Epoch  3 Batch :  321  Training Loss : 20.033287450875625525.576736450195\n",
            "18.213934536637932\n",
            "Epoch  3 Batch :  322  Training Loss : 20.0276372865456723755.010507202147\n",
            "16.579990166083917\n",
            "Epoch  3 Batch :  323  Training Loss : 20.0169634564513625925.735093688963\n",
            "20.135047224363916\n",
            "Epoch  3 Batch :  324  Training Loss : 20.01732791252516529335.508728027344\n",
            "20.256512388162424\n",
            "Epoch  3 Batch :  325  Training Loss : 20.0180638647578929596.25418701172\n",
            "20.884375\n",
            "Epoch  3 Batch :  326  Training Loss : 20.0207212608782726741.918923950194\n",
            "18.883227848101264\n",
            "Epoch  3 Batch :  327  Training Loss : 20.0172426877505135243.16093444824\n",
            "25.711583227040812\n",
            "Epoch  3 Batch :  328  Training Loss : 20.03460348207761428162.15440979004\n",
            "19.88889072410148\n",
            "Epoch  3 Batch :  329  Training Loss : 20.03416058615671622484.17919464111\n",
            "15.651883771929828\n",
            "Epoch  3 Batch :  330  Training Loss : 20.02088095944693431745.0774597168\n",
            "21.868930623259054\n",
            "Epoch  3 Batch :  331  Training Loss : 20.0264641910596626240.926358032226\n",
            "18.25151757538569\n",
            "Epoch  3 Batch :  332  Training Loss : 20.02111796631365742030.811248779304\n",
            "28.95318103448276\n",
            "Epoch  3 Batch :  333  Training Loss : 20.04794097853037727861.70637359619\n",
            "20.29656546966374\n",
            "Epoch  3 Batch :  334  Training Loss : 20.0486853632343724534.36091918945\n",
            "16.77169462943989\n",
            "Epoch  3 Batch :  335  Training Loss : 20.03890330134244431804.33936767578\n",
            "21.995954241071427\n",
            "Epoch  3 Batch :  336  Training Loss : 20.04472785771068625603.62557220459\n",
            "18.34185483870968\n",
            "Epoch  3 Batch :  337  Training Loss : 20.03967482204599527838.37151489258\n",
            "19.82053997349823\n",
            "Epoch  3 Batch :  338  Training Loss : 20.0390264940917126372.900535583496\n",
            "18.09964689875605\n",
            "Epoch  3 Batch :  339  Training Loss : 20.0333056103296632101.340701293942\n",
            "21.993294808884297\n",
            "Epoch  3 Batch :  340  Training Loss : 20.03907028444305624489.625595092773\n",
            "17.0063322945927\n",
            "Epoch  3 Batch :  341  Training Loss : 20.03017662464877426678.04742736816\n",
            "18.681496710526314\n",
            "Epoch  3 Batch :  342  Training Loss : 20.02623311612794827528.65289916992\n",
            "19.805901650432897\n",
            "Epoch  3 Batch :  343  Training Loss : 20.0255907503387533730.00701293945\n",
            "23.149633805841923\n",
            "Epoch  3 Batch :  344  Training Loss : 20.03467227084893340836.021368408205\n",
            "28.491859984331477\n",
            "Epoch  3 Batch :  345  Training Loss : 20.05918585842424724392.011404418947\n",
            "16.94602778262387\n",
            "Epoch  3 Batch :  346  Training Loss : 20.05018829173118227647.37239532471\n",
            "19.099459635416668\n",
            "Epoch  3 Batch :  347  Training Loss : 20.04744843969569234870.99323730468\n",
            "24.178907329074587\n",
            "Epoch  3 Batch :  348  Training Loss : 20.05932044799850530818.697717285155\n",
            "21.355517646036162\n",
            "Epoch  3 Batch :  349  Training Loss : 20.0630344800845724078.35440368652\n",
            "17.102323263642806\n",
            "Epoch  3 Batch :  350  Training Loss : 20.05457530518045328163.134492492674\n",
            "20.39173658470926\n",
            "Epoch  3 Batch :  351  Training Loss : 20.05553587862640424217.38206176758\n",
            "17.35246740107914\n",
            "Epoch  3 Batch :  352  Training Loss : 20.04785670681519232291.602575683595\n",
            "23.51911911231884\n",
            "Epoch  3 Batch :  353  Training Loss : 20.0576903113633629579.276141357423\n",
            "20.344764764118455\n",
            "Epoch  3 Batch :  354  Training Loss : 20.05850125614515330626.58832550049\n",
            "21.472436403508773\n",
            "Epoch  3 Batch :  355  Training Loss : 20.0624841720532219703.4027053833\n",
            "13.659481285063112\n",
            "Epoch  3 Batch :  356  Training Loss : 20.0444982088875227177.541189575193\n",
            "19.071528021784964\n",
            "Epoch  3 Batch :  357  Training Loss : 20.04177280220095630832.158404541016\n",
            "22.732671393171806\n",
            "Epoch  3 Batch :  358  Training Loss : 20.0492892787120530558.537448120114\n",
            "22.345013686131388\n",
            "Epoch  3 Batch :  359  Training Loss : 20.0556840542201824432.80880584717\n",
            "17.946714187956204\n",
            "Epoch  3 Batch :  360  Training Loss : 20.04982580459166627846.27381286621\n",
            "19.465230286113048\n",
            "Epoch  3 Batch :  361  Training Loss : 20.04820642642413725620.116586303713\n",
            "18.83205022075055\n",
            "Epoch  3 Batch :  362  Training Loss : 20.04484687889465333488.898760986325\n",
            "23.11431080846634\n",
            "Epoch  3 Batch :  363  Training Loss : 20.05330270239209522505.821524047853\n",
            "15.909503623836793\n",
            "Epoch  3 Batch :  364  Training Loss : 20.04191863898947225831.82050476074\n",
            "18.65669116333095\n",
            "Epoch  3 Batch :  365  Training Loss : 20.03812349522054226824.793087768558\n",
            "18.78146842560554\n",
            "Epoch  3 Batch :  366  Training Loss : 20.0346900114237830303.297232055666\n",
            "21.59457707439199\n",
            "Epoch  3 Batch :  367  Training Loss : 20.0389403848923637879.207275390625\n",
            "27.050846586847744\n",
            "Epoch  3 Batch :  368  Training Loss : 20.05799447783245523642.158557128903\n",
            "16.14865796232877\n",
            "Epoch  3 Batch :  369  Training Loss : 20.0474000699313620735.188204956055\n",
            "15.383590318448023\n",
            "Epoch  3 Batch :  370  Training Loss : 20.03479517871113634965.270904541016\n",
            "25.35773755450581\n",
            "Epoch  3 Batch :  371  Training Loss : 20.04914273228470739784.515179443355\n",
            "26.923705508474573\n",
            "Epoch  3 Batch :  372  Training Loss : 20.06762273974758426362.75252838135\n",
            "18.41843903293623\n",
            "Epoch  3 Batch :  373  Training Loss : 20.06320133570787326922.228341674803\n",
            "18.098597892590075\n",
            "Epoch  3 Batch :  374  Training Loss : 20.05794838532520824759.045288085938\n",
            "17.42971712605337\n",
            "Epoch  3 Batch :  375  Training Loss : 20.05093976863381423775.176223754883\n",
            "16.486341109764542\n",
            "Epoch  3 Batch :  376  Training Loss : 20.0414594530517229406.378283691407\n",
            "21.70606997784343\n",
            "Epoch  3 Batch :  377  Training Loss : 20.0458748655843226134.460894775395\n",
            "18.168083158995817\n",
            "Epoch  3 Batch :  378  Training Loss : 20.04090716265683627453.78735961914\n",
            "19.34192653508772\n",
            "Epoch  3 Batch :  379  Training Loss : 20.0390628865946526166.599340820307\n",
            "17.857629494863012\n",
            "Epoch  3 Batch :  380  Training Loss : 20.03332227240588329754.063549804687\n",
            "21.81391826923077\n",
            "Epoch  3 Batch :  381  Training Loss : 20.0379957527125123322.7076751709\n",
            "16.4665589838256\n",
            "Epoch  3 Batch :  382  Training Loss : 20.02864644179919323215.86411590576\n",
            "16.814841472303208\n",
            "Epoch  3 Batch :  383  Training Loss : 20.02025530610860236863.07175292968\n",
            "25.824777572233966\n",
            "Epoch  3 Batch :  384  Training Loss : 20.0353712495099726149.615032958984\n",
            "18.357843331571527\n",
            "Epoch  3 Batch :  385  Training Loss : 20.03101403413870331253.48964233398\n",
            "21.720227923317143\n",
            "Epoch  3 Batch :  386  Training Loss : 20.0353902359241428652.507476806644\n",
            "19.50191031313819\n",
            "Epoch  3 Batch :  387  Training Loss : 20.0340117348316721846.90457611084\n",
            "15.948114575779549\n",
            "Epoch  3 Batch :  388  Training Loss : 20.02348107205060825893.97698669434\n",
            "18.24709649122807\n",
            "Epoch  3 Batch :  389  Training Loss : 20.01891453071173323863.65061340332\n",
            "17.295559745303468\n",
            "Epoch  3 Batch :  390  Training Loss : 20.01193156972350826887.53190917969\n",
            "18.907768236543912\n",
            "Epoch  3 Batch :  391  Training Loss : 20.00910762257982627570.895373535157\n",
            "19.609469461237552\n",
            "Epoch  3 Batch :  392  Training Loss : 20.0080881374743625683.758709716796\n",
            "18.274975741002116\n",
            "Epoch  3 Batch :  393  Training Loss : 20.00367818226705225617.974609375\n",
            "19.036973745353162\n",
            "Epoch  3 Batch :  394  Training Loss : 20.00122461770635627158.39292297363\n",
            "19.126999209971913\n",
            "Epoch  3 Batch :  395  Training Loss : 19.99901138882601320798.232339477538\n",
            "14.318012004442927\n",
            "Epoch  3 Batch :  396  Training Loss : 19.9846654307846428497.736328125\n",
            "20.953369784040994\n",
            "Epoch  3 Batch :  397  Training Loss : 19.98710549212785629775.622607421876\n",
            "20.423531314699794\n",
            "Epoch  3 Batch :  398  Training Loss : 19.98820203942075223136.36575164795\n",
            "17.221310114146707\n",
            "Epoch  3 Batch :  399  Training Loss : 19.98126747319199529905.927770996095\n",
            "21.435797491039427\n",
            "Epoch  3 Batch :  400  Training Loss : 19.98490379823661224855.643986511226\n",
            "17.34556930478502\n",
            "Epoch  3 Batch :  401  Training Loss : 19.97832191670680829566.149432373044\n",
            "21.261571606254442\n",
            "Epoch  3 Batch :  402  Training Loss : 19.98151408011364424284.80165405273\n",
            "17.26332289445629\n",
            "Epoch  3 Batch :  403  Training Loss : 19.97476918883409521290.509962463377\n",
            "15.079195765558698\n",
            "Epoch  3 Batch :  404  Training Loss : 19.9626514328358928561.041845703126\n",
            "19.399748807901908\n",
            "Epoch  3 Batch :  405  Training Loss : 19.9612615498113623538.336781311034\n",
            "16.186727346973868\n",
            "Epoch  3 Batch :  406  Training Loss : 19.95196466753836322259.978897094727\n",
            "15.491543989547038\n",
            "Epoch  3 Batch :  407  Training Loss : 19.94100540297327531041.461865234374\n",
            "21.476837725381415\n",
            "Epoch  3 Batch :  408  Training Loss : 19.94476969788113525998.349487304688\n",
            "18.601068490701\n",
            "Epoch  3 Batch :  409  Training Loss : 19.94148436485624722703.898966979978\n",
            "16.0125\n",
            "Epoch  3 Batch :  410  Training Loss : 19.93190147616147525212.96407928467\n",
            "18.193240095875545\n",
            "Epoch  3 Batch :  411  Training Loss : 19.92767115650141526731.77867126465\n",
            "19.303719917385056\n",
            "Epoch  3 Batch :  412  Training Loss : 19.92615671174627725492.824649047852\n",
            "18.27840909090909\n",
            "Epoch  3 Batch :  413  Training Loss : 19.92216700806386223057.21892242432\n",
            "17.035592202512934\n",
            "Epoch  3 Batch :  414  Training Loss : 19.91519460515190730684.403619384764\n",
            "21.40355607192495\n",
            "Epoch  3 Batch :  415  Training Loss : 19.91878101832485728617.915908813477\n",
            "20.204551056338026\n",
            "Epoch  3 Batch :  416  Training Loss : 19.9194679655316226774.00171508789\n",
            "18.58284416491964\n",
            "Epoch  3 Batch :  417  Training Loss : 19.91626263267643227266.99753112793\n",
            "19.23332779255319\n",
            "Epoch  3 Batch :  418  Training Loss : 19.91462881726944227556.64981384277\n",
            "20.01671408412236\n",
            "Epoch  3 Batch :  419  Training Loss : 19.9148724575244629911.12293395996\n",
            "20.960430891106444\n",
            "Epoch  3 Batch :  420  Training Loss : 19.9173618823663225654.70144958496\n",
            "19.009573632668143\n",
            "Epoch  3 Batch :  421  Training Loss : 19.91520561573995734954.65315856934\n",
            "24.613835187146893\n",
            "Epoch  3 Batch :  422  Training Loss : 19.9263398090371322323.195751953124\n",
            "15.413836805555558\n",
            "Epoch  3 Batch :  423  Training Loss : 19.91567195323693826479.81177062988\n",
            "17.925823552090474\n",
            "Epoch  3 Batch :  424  Training Loss : 19.9109789145549931921.235273742677\n",
            "23.18244445447977\n",
            "Epoch  3 Batch :  425  Training Loss : 19.91867648053128229307.77614135742\n",
            "20.602163546061885\n",
            "Epoch  3 Batch :  426  Training Loss : 19.92028091026257329131.678396606447\n",
            "20.605867662659126\n",
            "Epoch  3 Batch :  427  Training Loss : 19.92188649984664323265.194107055668\n",
            "16.561658653846155\n",
            "Epoch  3 Batch :  428  Training Loss : 19.91403550020645227267.833850097657\n",
            "18.74283107069672\n",
            "Epoch  3 Batch :  429  Training Loss : 19.91130541995118429304.12841796875\n",
            "20.39310013956734\n",
            "Epoch  3 Batch :  430  Training Loss : 19.91242587278749826783.819586181642\n",
            "19.759348652859238\n",
            "Epoch  3 Batch :  431  Training Loss : 19.91207070522385832276.314431762694\n",
            "22.390086132271467\n",
            "Epoch  3 Batch :  432  Training Loss : 19.9178068520457329708.663909912106\n",
            "20.37465837045922\n",
            "Epoch  3 Batch :  433  Training Loss : 19.91886193638386631591.040815734865\n",
            "23.662499999999994\n",
            "Epoch  3 Batch :  434  Training Loss : 19.92748783053966528926.45447692871\n",
            "20.956659226190474\n",
            "Epoch  3 Batch :  435  Training Loss : 19.92985374179403428744.767848205563\n",
            "20.35936279296875\n",
            "Epoch  3 Batch :  436  Training Loss : 19.9308388542967329934.6576171875\n",
            "21.170337843794076\n",
            "Epoch  3 Batch :  437  Training Loss : 19.93367523642372832512.070880126954\n",
            "22.467270416953824\n",
            "Epoch  3 Batch :  438  Training Loss : 19.93945970030621626789.48695678711\n",
            "18.392241083676268\n",
            "Epoch  3 Batch :  439  Training Loss : 19.93593528432300529936.2203125\n",
            "20.973912901256107\n",
            "Epoch  3 Batch :  440  Training Loss : 19.9382943243614938114.863806152345\n",
            "26.10580741729842\n",
            "Epoch  3 Batch :  441  Training Loss : 19.95227961482166328075.272216796875\n",
            "19.598070499296764\n",
            "Epoch  3 Batch :  442  Training Loss : 19.9514782367322433639.68637695313\n",
            "24.541759386446888\n",
            "Epoch  3 Batch :  443  Training Loss : 19.96184004519660635465.222076416016\n",
            "26.218039143279178\n",
            "Epoch  3 Batch :  444  Training Loss : 19.975930583705831140.08927307129\n",
            "22.249403591851323\n",
            "Epoch  3 Batch :  445  Training Loss : 19.98103951181399326037.347074890138\n",
            "18.232899427816903\n",
            "Epoch  3 Batch :  446  Training Loss : 19.97711991521310527994.24811096191\n",
            "19.686543188202243\n",
            "Epoch  3 Batch :  447  Training Loss : 19.97646985542113325183.349432373045\n",
            "17.66522286991585\n",
            "Epoch  3 Batch :  448  Training Loss : 19.97131082197134740118.38479614258\n",
            "28.830733666306696\n",
            "Epoch  3 Batch :  449  Training Loss : 19.9910422759676429271.71259460449\n",
            "21.62000023216939\n",
            "Epoch  3 Batch :  450  Training Loss : 19.99466218253697726316.525689697264\n",
            "18.993783747300217\n",
            "Epoch  3 Batch :  451  Training Loss : 19.99244293988678639358.897665405275\n",
            "27.46998072880168\n",
            "Epoch  3 Batch :  452  Training Loss : 20.00898616508350329540.483181762695\n",
            "20.63581092877095\n",
            "Epoch  3 Batch :  453  Training Loss : 20.0103698842086421593.476879882808\n",
            "15.222773164335663\n",
            "Epoch  3 Batch :  454  Training Loss : 19.99982451698424828734.068341064452\n",
            "19.921290545203586\n",
            "Epoch  3 Batch :  455  Training Loss : 19.99965191484846729235.665832519535\n",
            "19.990259533898303\n",
            "Epoch  3 Batch :  456  Training Loss : 19.99963131752182528310.01590270996\n",
            "20.113117038216558\n",
            "Epoch  3 Batch :  457  Training Loss : 19.99987964513822234850.27043457031\n",
            "25.13085485910404\n",
            "Epoch  3 Batch :  458  Training Loss : 20.01108264778880434812.619104003905\n",
            "24.330761312413312\n",
            "Epoch  3 Batch :  459  Training Loss : 20.02049371241761524546.1669052124\n",
            "17.501668830587395\n",
            "Epoch  3 Batch :  460  Training Loss : 20.01501800615276527353.7192199707\n",
            "19.55183908815132\n",
            "Epoch  3 Batch :  461  Training Loss : 20.0140132796495124584.101045227053\n",
            "17.59961273280802\n",
            "Epoch  3 Batch :  462  Training Loss : 20.00878730443989521209.179385375974\n",
            "14.941322761854213\n",
            "Epoch  3 Batch :  463  Training Loss : 19.99784245661573226714.657064819334\n",
            "18.484696818654648\n",
            "Epoch  3 Batch :  464  Training Loss : 19.99458136687874826856.7017868042\n",
            "19.58810457966226\n",
            "Epoch  3 Batch :  465  Training Loss : 19.99370722325032229197.268090820315\n",
            "20.313068083448755\n",
            "Epoch  3 Batch :  466  Training Loss : 19.99439254698465637364.18497314453\n",
            "26.052260083449237\n",
            "Epoch  3 Batch :  467  Training Loss : 20.00736442607772532480.56499023438\n",
            "22.618379991319443\n",
            "Epoch  3 Batch :  468  Training Loss : 20.01294351916584829659.26104736328\n",
            "20.38681492047026\n",
            "Epoch  3 Batch :  469  Training Loss : 20.0137406863328124409.365081787106\n",
            "17.195378942239547\n",
            "Epoch  3 Batch :  470  Training Loss : 20.0077441719836720952.411598205566\n",
            "14.956893119183965\n",
            "Epoch  3 Batch :  471  Training Loss : 19.99702049671233431011.17932128906\n",
            "20.825457902202935\n",
            "Epoch  3 Batch :  472  Training Loss : 19.9987756607070229420.314331054688\n",
            "20.906672807173297\n",
            "Epoch  3 Batch :  473  Training Loss : 20.00069510499130524036.13228759766\n",
            "17.47173305695746\n",
            "Epoch  3 Batch :  474  Training Loss : 19.9953597420207743467.054516601565\n",
            "30.466995882971265\n",
            "Epoch  3 Batch :  475  Training Loss : 20.01740529179119234469.60944213867\n",
            "24.13764194619148\n",
            "Epoch  3 Batch :  476  Training Loss : 20.02606125114917638505.58196411133\n",
            "26.982595061188814\n",
            "Epoch  3 Batch :  477  Training Loss : 20.0406451794721131548.910813903807\n",
            "21.809993671438544\n",
            "Epoch  3 Batch :  478  Training Loss : 20.04434674535488634967.61852416992\n",
            "24.43181591932133\n",
            "Epoch  3 Batch :  479  Training Loss : 20.05350638872433731476.801232910155\n",
            "22.164642065602838\n",
            "Epoch  3 Batch :  480  Training Loss : 20.05790458805116630319.387893676758\n",
            "21.39026569092827\n",
            "Epoch  3 Batch :  481  Training Loss : 20.0606745695540323743.22874145508\n",
            "16.774597202549575\n",
            "Epoch  3 Batch :  482  Training Loss : 20.0538569816556834073.34952392578\n",
            "23.873842309706703\n",
            "Epoch  3 Batch :  483  Training Loss : 20.0617658539704923310.094296264648\n",
            "17.397206263899186\n",
            "Epoch  3 Batch :  484  Training Loss : 20.0562605655612521363.296130371094\n",
            "15.03272470661451\n",
            "Epoch  3 Batch :  485  Training Loss : 20.04590275966651426601.494892883304\n",
            "19.232515794223826\n",
            "Epoch  3 Batch :  486  Training Loss : 20.0442291239351521636.430699157714\n",
            "21.5593875250501\n",
            "Epoch  3 Batch :  487  Training Loss : 20.04734033215099Validating :  25.043103368924893\n",
            "25.09820162923417\n",
            "Validating :  26.341928146258503\n",
            "25.975977940467423\n",
            "Validating :  29.304391084558826\n",
            "27.08730905283613\n",
            "Validating :  29.41674456908832\n",
            "27.642906780723962\n",
            "Validating :  31.970514008620686\n",
            "28.460893959061927\n",
            "Validating :  44.22094338389121\n",
            "31.03857311994119\n",
            "Validating :  37.26918704105474\n",
            "31.93213718429634\n",
            "Validating :  42.31539867688022\n",
            "33.215394121173986\n",
            "Validating :  32.418649400684934\n",
            "33.12140305538905\n",
            "Validating :  29.896215349517906\n",
            "32.830749922715164\n",
            "Validating :  27.366716075249645\n",
            "32.35468931422427\n",
            "Validating :  30.365340517241382\n",
            "32.17214040226305\n",
            "Validating :  25.937229136819482\n",
            "31.658385346523616\n",
            "Validating :  34.215024862825786\n",
            "31.86619022734218\n",
            "Validating :  32.7560477811174\n",
            "31.922773316971515\n",
            "Validating :  36.77142349137931\n",
            "32.21524963444528\n",
            "Validating :  41.40148703835227\n",
            "32.73667122805745\n",
            "Validating :  30.037219465307583\n",
            "32.581727503815515\n",
            "Validating :  58.504638894157814\n",
            "33.952940816449136\n",
            "Validating :  28.04379930750351\n",
            "33.649053363196806\n",
            "Validating :  34.489535179363905\n",
            "33.69960601149414\n",
            "Validating :  27.234802496580027\n",
            "33.40091010181859\n",
            "Validating :  31.46992078690808\n",
            "33.31560239793971\n",
            "Validating :  41.15341357970506\n",
            "33.622657174985996\n",
            "Validating :  27.064520743919882\n",
            "33.3606806090166\n",
            "Validating :  31.645666575292395\n",
            "33.29579182701776\n",
            "Validating :  30.383252290802215\n",
            "33.192841134958755\n",
            "Validating :  30.292254339060307\n",
            "33.08946335443002\n",
            "Validating :  27.588630462034384\n",
            "32.904851758669125\n",
            "Validating :  35.47592075892857\n",
            "32.98636712446398\n",
            "Validating :  33.999317797364675\n",
            "33.015785985201795\n",
            "Validating :  36.35061616443453\n",
            "33.1273680428629\n",
            "Validating :  28.181560283687944\n",
            "32.97903407412366\n",
            "Validating :  30.51672329466759\n",
            "32.91885837466679\n",
            "Validating :  40.858979301948054\n",
            "33.1473079490888\n",
            "Validating :  31.53352491258741\n",
            "33.09889860823416\n",
            "Validating :  28.46901572296238\n",
            "32.9703169724931\n",
            "Validating :  31.339156600140058\n",
            "32.9333864082752\n",
            "Validating :  21.866705203662182\n",
            "32.65154088296432\n",
            "Validating :  32.041526958154506\n",
            "32.63860058715095\n",
            "Validating :  46.68407619089012\n",
            "32.98049116252964\n",
            "Validating :  28.295522881996977\n",
            "32.86659036167119\n",
            "Validating :  30.99475809871099\n",
            "32.82631215069484\n",
            "Validating :  36.761808685852984\n",
            "32.909904227138504\n",
            "Validating :  30.748526084357916\n",
            "32.85682713942553\n",
            "Validating :  27.725552801724135\n",
            "32.74209031443053\n",
            "Validating :  36.40794307408708\n",
            "32.81139358088479\n",
            "Validating :  32.21888078703704\n",
            "32.80505747329691\n",
            "Validating :  41.40008037065053\n",
            "32.978622306578295\n",
            "Validating :  24.45499888392857\n",
            "32.808336199732445\n",
            "Validating :  30.671234309623433\n",
            "32.76513575537118\n",
            "Validating :  29.77936789772728\n",
            "32.70997211431509\n",
            "Validating :  25.43973770775623\n",
            "32.570804800536926\n",
            "Validating :  29.330659847122302\n",
            "32.50595357837803\n",
            "Validating :  30.747507072135782\n",
            "32.46921042247182\n",
            "Validating :  52.99276916058394\n",
            "32.850575477655774\n",
            "Validating :  24.915900287256267\n",
            "32.7125016544943\n",
            "Validating :  39.21342472202296\n",
            "32.82698105288438\n",
            "Validating :  28.007167070761493\n",
            "32.74099730494683\n",
            "Validating :  32.198854095997284\n",
            "32.72762677800775\n",
            "Validating :  33.19348768248175\n",
            "32.735146631144396\n",
            "Validating :  28.659353827913282\n",
            "32.66639262016694\n",
            "Validating :  36.65360944323144\n",
            "32.7304048669079\n",
            "Validating :  30.577122724089634\n",
            "32.697237566023304\n",
            "Validating :  28.75994706284153\n",
            "32.63133133392307\n",
            "Validating :  44.93430082312405\n",
            "32.82619801164474\n",
            "Validating :  38.71592583272328\n",
            "32.91522378294665\n",
            "Validating :  30.52639439655172\n",
            "32.87342293560615\n",
            "Validating :  35.042547104779416\n",
            "32.90132317931966\n",
            "Validating :  27.801201923076924\n",
            "32.828774044713214\n",
            "Validating :  32.20848321600275\n",
            "32.821012393589136\n",
            "Validating :  27.150825829172607\n",
            "32.74119367912059\n",
            "Validating :  36.038623316643644\n",
            "32.78367482035114\n",
            "Validating :  28.63344853206052\n",
            "32.72495958356627\n",
            "Validating :  28.085319840604022\n",
            "32.66101126898449\n",
            "Validating :  29.374941918731782\n",
            "32.61997994323504\n",
            "Validating :  33.103888056506854\n",
            "32.626590696173814\n",
            "Validating :  45.1647889058023\n",
            "32.786233098349825\n",
            "Validating :  42.81803755928184\n",
            "32.91238736254175\n",
            "Validating :  26.416653797238375\n",
            "32.83214099616405\n",
            "Validating :  25.677010601158038\n",
            "32.74532329744009\n",
            "Validating :  31.689870037012113\n",
            "32.73273441824651\n",
            "Validating :  32.32428120286576\n",
            "32.73059359055934\n",
            "Validating :  33.83617011278196\n",
            "32.747791618213796\n",
            "Validating :  36.216892286278735\n",
            "32.77985635002115\n",
            "Validating :  31.764195478723405\n",
            "32.76238556964403\n",
            "Validating :  44.79317912868163\n",
            "32.90519185593788\n",
            "Validating :  31.222772412536447\n",
            "32.885908346037375\n",
            "Validating :  41.75497509696756\n",
            "32.98581688128498\n",
            "Validating :  34.67145086146273\n",
            "33.00267517668124\n",
            "Validating :  28.99591908028455\n",
            "32.95647220338854\n",
            "Validating :  27.61686049277017\n",
            "32.89773887605041\n",
            "Validating :  31.86153125\n",
            "32.89099550452379\n",
            "Validating :  32.69517259133237\n",
            "32.88970866599337\n",
            "Validating :  38.922014352176966\n",
            "32.95134378285932\n",
            "Validating :  30.395049504950492\n",
            "32.92592945665364\n",
            "Validating :  31.867945043103447\n",
            "32.913791108629134\n",
            "Validating :  26.358795470756064\n",
            "32.845391834292194\n",
            "Validating :  29.746039511014683\n",
            "32.81597751988463\n",
            "Validating :  31.42515176128591\n",
            "32.7991547936769\n",
            "Validating :  32.933954822902336\n",
            "32.803621608029886\n",
            "Validating :  27.286391410614527\n",
            "32.747202941262586\n",
            "Validating :  29.15148069992413\n",
            "32.71306725712743\n",
            "Validating :  30.797004444648092\n",
            "32.692782225643235\n",
            "Validating :  40.23650025579809\n",
            "32.76577675851342\n",
            "Validating :  31.447830869013064\n",
            "32.751025079940895\n",
            "Validating :  33.184796091573816\n",
            "32.75588835163735\n",
            "Validating :  40.2421736521418\n",
            "32.82966005291945\n",
            "Validating :  34.26440332861189\n",
            "32.83896402635018\n",
            "Validating :  42.83424926925723\n",
            "32.92808038087805\n",
            "Validating :  39.90951192010309\n",
            "32.98985043864562\n",
            "Validating :  26.488836230187317\n",
            "32.92877930013157\n",
            "Validating :  35.44035408266129\n",
            "32.95455896013374\n",
            "Validating :  29.90046768707483\n",
            "32.92755598888778\n",
            "Validating :  29.7301834039028\n",
            "32.90173409346671\n",
            "Validating :  27.742012255788712\n",
            "32.8608818825145\n",
            "Validating :  32.92802396616542\n",
            "32.86038140597864\n",
            "Validating :  27.46397875367107\n",
            "32.81406973212574\n",
            "Validating :  30.135190496575344\n",
            "32.787280845375975\n",
            "Validating :  30.286793206146406\n",
            "32.771064747156565\n",
            "Validating :  29.420615846389648\n",
            "32.74581997867229\n",
            "Validating :  28.734511746771876\n",
            "32.71503072496042\n",
            "Validating :  36.2927869431516\n",
            "32.74403771432839\n",
            "Validating :  27.079453812316714\n",
            "32.69871824727322\n",
            "Validating :  31.939579861111106\n",
            "32.69192386240614\n",
            "Validating :  35.26164772727273\n",
            "32.71400999797631\n",
            "Validating :  34.72367709847383\n",
            "32.729150840743685\n",
            "Validating :  30.667178001519755\n",
            "32.71196583591182\n",
            "Validating :  32.10006674544818\n",
            "32.70937369838179\n",
            "Validating :  30.778734570584145\n",
            "32.695073272759814\n",
            "Validating :  32.38747351694916\n",
            "32.690563849353225\n",
            "Validating :  30.673342468175388\n",
            "32.67437571830984\n",
            "Validating :  36.80823590860597\n",
            "32.704497311803124\n",
            "Validating :  35.907030141843975\n",
            "32.72952552533841\n",
            "Validating :  31.613632160470406\n",
            "32.72105183333933\n",
            "Validating :  27.323899982710927\n",
            "32.680655165040335\n",
            "Validating :  32.41768744885434\n",
            "32.677518668545545\n",
            "Validating :  39.15325360082305\n",
            "32.727560485401874\n",
            "Validating :  38.63082196302817\n",
            "32.77243711955036\n",
            "Validating :  29.04292601683029\n",
            "32.74815570196131\n",
            "Validating :  28.493000252016135\n",
            "32.71659111643779\n",
            "Validating :  30.361592475453175\n",
            "32.70189196925701\n",
            "Validating :  26.686981059229648\n",
            "32.66096335072047\n",
            "Validating :  37.14703757856145\n",
            "32.68955664538784\n",
            "Validating :  31.3249786834925\n",
            "32.676933644531\n",
            "Validating :  46.80629549893467\n",
            "32.77091002608601\n",
            "Validating :  28.111683336195057\n",
            "32.739802682917784\n",
            "Validating :  30.189922975352115\n",
            "32.71972569783622\n",
            "Validating :  31.368024553571427\n",
            "32.71037463550654\n",
            "Validating :  32.4922470868644\n",
            "32.708109835088095\n",
            "Validating :  32.30874957884097\n",
            "32.705461271958775\n",
            "Validating :  28.712880236037233\n",
            "32.679811218141644\n",
            "Validating :  36.83620854591837\n",
            "32.70712103615794\n",
            "Validating :  28.45256504828326\n",
            "32.679436590922876\n",
            "Validating :  36.96610175142247\n",
            "32.7042275199054\n",
            "Validating :  32.07282487383541\n",
            "32.70427408642108\n",
            "Validating :  28.622656250000002\n",
            "32.677626051087934\n",
            "Validating :  28.428358388849926\n",
            "32.65123869578386\n",
            "Validating :  34.160362738419614\n",
            "32.6608055053613\n",
            "Validating :  46.22817355225989\n",
            "32.74574444688632\n",
            "Validating :  32.46989199112022\n",
            "32.7424151008305\n",
            "Validating :  32.18420472756411\n",
            "32.74075496510028\n",
            "Validating :  42.68893502058383\n",
            "32.800143912531155\n",
            "Validating :  26.627151564774383\n",
            "32.760792375056816\n",
            "Validating :  24.25786139758179\n",
            "32.70932421902927\n",
            "Validating :  38.23327747989276\n",
            "32.74102645203384\n",
            "Validating :  28.09094337066474\n",
            "32.714435865661294\n",
            "Validating :  36.13902693812233\n",
            "32.73429728038001\n",
            "Validating :  38.2838743622449\n",
            "32.76739300980215\n",
            "Validating :  28.87777789572011\n",
            "32.743463159916324\n",
            "Validating :  23.373261398565575\n",
            "32.68997684307383\n",
            "Validating :  32.04043544738605\n",
            "32.68551020311667\n",
            "Validating :  35.34945212482219\n",
            "32.70165689829554\n",
            "Validating :  39.91097575111276\n",
            "32.74072157372401\n",
            "Validating :  31.5455184853142\n",
            "32.732822736842934\n",
            "Validating :  30.759227858744392\n",
            "32.7211514961241\n",
            "Validating :  26.020414545294944\n",
            "32.68412320297961\n",
            "Validating :  24.121039094650204\n",
            "32.63534880589131\n",
            "Validating :  41.20997388639503\n",
            "32.68214239637277\n",
            "Validating :  28.461893918161433\n",
            "32.65939088618406\n",
            "Validating :  32.82743584857724\n",
            "32.659010016680824\n",
            "Validating :  25.858361366855522\n",
            "32.62274934376518\n",
            "Validating :  28.312362865691487\n",
            "32.5983630498367\n",
            "Validating :  27.93068210767663\n",
            "32.57342277624465\n",
            "Validating :  39.493666958041956\n",
            "32.61409132587598\n",
            "Validating :  32.02605996621622\n",
            "32.610786762659274\n",
            "Validating :  23.374072916666666\n",
            "32.55991511256187\n",
            "Validating :  27.718965063202248\n",
            "32.53405381363695\n",
            "Validating :  30.6484888229927\n",
            "32.52669568726232\n",
            "Validating :  32.91903746290801\n",
            "32.528558443835685\n",
            "Validating :  28.219213687407954\n",
            "32.50606093468213\n",
            "Validating :  34.361382865646256\n",
            "32.515071771385\n",
            "Validating :  31.90818292025862\n",
            "32.51267046555797\n",
            "Validating :  25.739350665983608\n",
            "32.478431594995826\n",
            "Validating :  31.329830452127652\n",
            "32.47388902094075\n",
            "Validating :  28.87953587962963\n",
            "32.45443757823187\n",
            "Validating :  29.128059234397675\n",
            "32.43803983764814\n",
            "Validating :  32.84663486168033\n",
            "32.4391518542975\n",
            "Validating :  34.641256602112676\n",
            "32.44988781731468\n",
            "Validating :  30.453499955229226\n",
            "32.439229190146094\n",
            "Validating :  26.3163475177305\n",
            "32.40896216081366\n",
            "Validating :  28.466743758852694\n",
            "32.39084163906209\n",
            "Validating :  32.25079794337607\n",
            "32.38901941500225\n",
            "Validating :  28.527604166666666\n",
            "32.37016132551395\n",
            "Validating :  25.957585164327917\n",
            "32.34129968701827\n",
            "Validating :  35.97235280797101\n",
            "32.359654500694624\n",
            "Validating :  53.65023220168375\n",
            "32.46468809370589\n",
            "Validating :  28.106538112914365\n",
            "32.44386452867692\n",
            "Validating :  27.956634442446045\n",
            "32.42424415263182\n",
            "Validating :  27.860926619080775\n",
            "32.40071099719616\n",
            "Validating :  50.658252702231515\n",
            "32.48616793022748\n",
            "Validating :  32.72172368185079\n",
            "32.488649053099984\n",
            "Validating :  31.6569779071835\n",
            "32.48373007760375\n",
            "Validating :  26.70889273277126\n",
            "32.45595652443464\n",
            "Validating :  35.377945354278076\n",
            "32.47050171658097\n",
            "Validating :  43.117837832084476\n",
            "32.5209696871603\n",
            "Validating :  28.303621049360796\n",
            "32.50010030104207\n",
            "Validating :  36.92245742565698\n",
            "32.518272168792365\n",
            "Validating :  29.614792290419164\n",
            "32.50658988569587\n",
            "Validating :  26.75828125\n",
            "32.481727575315944\n",
            "Validating :  29.95325366283383\n",
            "32.47002964228284\n",
            "Validating :  28.863522550207758\n",
            "32.454124618844894\n",
            "Validating :  28.89067037004249\n",
            "32.439022858868555\n",
            "Validating :  31.39921763073066\n",
            "32.434801316776095\n",
            "Validating :  35.60751564643347\n",
            "32.449049998357076\n",
            "Validating :  30.817632834383755\n",
            "32.44229074707533\n",
            "Validating :  31.214965596330277\n",
            "32.43520088185361\n",
            "Validating :  34.00824986645299\n",
            "32.44286684083634\n",
            "Validating :  56.61573484576757\n",
            "32.546963849691736\n",
            "Validating :  28.739544502393983\n",
            "32.530436775520705\n",
            "Validating :  29.921275436046514\n",
            "32.51779228684863\n",
            "Validating :  27.40622306034483\n",
            "32.495869185472564\n",
            "Validating :  38.139248457910014\n",
            "32.52049259319919\n",
            "Validating :  38.12492071211096\n",
            "32.54590659636312\n",
            "Validating :  26.282988945578232\n",
            "32.5197515498393\n",
            "Validating :  30.404783078457445\n",
            "32.51005082041168\n",
            "Validating :  27.971826940271814\n",
            "32.490307139574874\n",
            "Validating :  29.061065074060693\n",
            "32.47506576563843\n",
            "Validating :  26.89574284957627\n",
            "32.452103909729445\n",
            "Validating :  27.65030252659574\n",
            "32.43405702748884\n",
            "Validating :  35.45462167645028\n",
            "32.44688780491455\n",
            "Validating :  35.16554731267507\n",
            "32.45755538553237\n",
            "Validating :  25.99410792951542\n",
            "32.43092940715364\n",
            "Validating :  50.52548608319936\n",
            "32.50631571966682\n",
            "-----------------------------------------------------------------------------------------------------------------------------\n",
            "Train nll 20.04734033215099\n",
            "Validation nll 32.50631571966682\n",
            "30500.286318969727\n",
            "22.68206129807692\n",
            "Epoch  4 Batch :  1  Training Loss : 22.6820612980769244213.623413085945\n",
            "30.445003019323668\n",
            "Epoch  4 Batch :  2  Training Loss : 26.56353215870029432487.513192749026\n",
            "22.38071577484364\n",
            "Epoch  4 Batch :  3  Training Loss : 25.16926003074807731931.4822845459\n",
            "22.76834127761939\n",
            "Epoch  4 Batch :  4  Training Loss : 24.56903034246590430644.95635986328\n",
            "21.633533226723525\n",
            "Epoch  4 Batch :  5  Training Loss : 23.98193091931742830150.07594909668\n",
            "20.4435053228022\n",
            "Epoch  4 Batch :  6  Training Loss : 23.39219331989822626614.066229248045\n",
            "18.723999605539973\n",
            "Epoch  4 Batch :  7  Training Loss : 22.72530850356133330119.786572265628\n",
            "21.853837581051874\n",
            "Epoch  4 Batch :  8  Training Loss : 22.6163746382476528803.76891784668\n",
            "20.245288326300987\n",
            "Epoch  4 Batch :  9  Training Loss : 22.3529206035869127640.984771728516\n",
            "19.849844195156695\n",
            "Epoch  4 Batch :  10  Training Loss : 22.10261296274388735226.42319946289\n",
            "24.27886174502402\n",
            "Epoch  4 Batch :  11  Training Loss : 22.3004537611329927224.09705505371\n",
            "18.70346995357634\n",
            "Epoch  4 Batch :  12  Training Loss : 22.00070511050327233431.186535644534\n",
            "23.816251778093886\n",
            "Epoch  4 Batch :  13  Training Loss : 22.1403625464717822821.569236755367\n",
            "15.947165596807771\n",
            "Epoch  4 Batch :  14  Training Loss : 21.69799133578149524671.961883544922\n",
            "18.7284226750753\n",
            "Epoch  4 Batch :  15  Training Loss : 21.50002009173441726217.04962158203\n",
            "18.974949306056235\n",
            "Epoch  4 Batch :  16  Training Loss : 21.3422031676295350859.08530273438\n",
            "36.273515624999995\n",
            "Epoch  4 Batch :  17  Training Loss : 22.22051566512191228231.041104125976\n",
            "19.848426504222378\n",
            "Epoch  4 Batch :  18  Training Loss : 22.08873293396082723020.478900146485\n",
            "16.467359744094487\n",
            "Epoch  4 Batch :  19  Training Loss : 21.7928711871257648524.5139465332\n",
            "33.29680324708704\n",
            "Epoch  4 Batch :  20  Training Loss : 22.36806779012382423461.99658966064\n",
            "17.531030721895664\n",
            "Epoch  4 Batch :  21  Training Loss : 22.13773269163676732771.65801086426\n",
            "22.43139126712329\n",
            "Epoch  4 Batch :  22  Training Loss : 22.15108080870433624410.94927825928\n",
            "17.037880302503478\n",
            "Epoch  4 Batch :  23  Training Loss : 21.92876774321734233692.48607177735\n",
            "23.38427984429066\n",
            "Epoch  4 Batch :  24  Training Loss : 21.98941408076206425442.622412109376\n",
            "18.3039201117814\n",
            "Epoch  4 Batch :  25  Training Loss : 21.84199432200283724679.16714477539\n",
            "16.888288731190148\n",
            "Epoch  4 Batch :  26  Training Loss : 21.6514671838946622615.359469604493\n",
            "15.77432218309859\n",
            "Epoch  4 Batch :  27  Training Loss : 21.43379514682813521534.39058837891\n",
            "15.458796047925606\n",
            "Epoch  4 Batch :  28  Training Loss : 21.22040232186733345197.765612792966\n",
            "31.66127575069832\n",
            "Epoch  4 Batch :  29  Training Loss : 21.58043244010288220236.416229248047\n",
            "14.76187408958485\n",
            "Epoch  4 Batch :  30  Training Loss : 21.35314716175228328332.181668090823\n",
            "19.729523140138408\n",
            "Epoch  4 Batch :  31  Training Loss : 21.30077219331312626849.185311889647\n",
            "19.41820070992092\n",
            "Epoch  4 Batch :  32  Training Loss : 21.2419418344571227114.00706481934\n",
            "18.37301116984993\n",
            "Epoch  4 Batch :  33  Training Loss : 21.15500454159023724973.50770874024\n",
            "17.749218195528744\n",
            "Epoch  4 Batch :  34  Training Loss : 21.0548343549413728642.800692749024\n",
            "20.460448729420186\n",
            "Epoch  4 Batch :  35  Training Loss : 21.03785190849790620760.65873565674\n",
            "14.855744122397706\n",
            "Epoch  4 Batch :  36  Training Loss : 20.86612669221734633735.302825927734\n",
            "24.91030563186813\n",
            "Epoch  4 Batch :  37  Training Loss : 20.9754288257214223724.602128601073\n",
            "16.106102916098227\n",
            "Epoch  4 Batch :  38  Training Loss : 20.8472886702050224432.58783569336\n",
            "18.110432219592134\n",
            "Epoch  4 Batch :  39  Training Loss : 20.7771128637790525933.660749816896\n",
            "18.45177491103203\n",
            "Epoch  4 Batch :  40  Training Loss : 20.71897941496037322608.1840713501\n",
            "16.143776939655172\n",
            "Epoch  4 Batch :  41  Training Loss : 20.60738911068463627594.074029541014\n",
            "19.599018767580873\n",
            "Epoch  4 Batch :  42  Training Loss : 20.58338029299168826591.06057434082\n",
            "18.70190522074282\n",
            "Epoch  4 Batch :  43  Training Loss : 20.53962505875334424908.19767456055\n",
            "17.427144351464435\n",
            "Epoch  4 Batch :  44  Training Loss : 20.4688868608604138590.936318969725\n",
            "27.59703214158345\n",
            "Epoch  4 Batch :  45  Training Loss : 20.62729008932092626379.323526000975\n",
            "18.970815947024672\n",
            "Epoch  4 Batch :  46  Training Loss : 20.59127978187970223387.520367431644\n",
            "16.845634814418272\n",
            "Epoch  4 Batch :  47  Training Loss : 20.51158520810392728262.817581176754\n",
            "20.735235805860807\n",
            "Epoch  4 Batch :  48  Training Loss : 20.51624459555719723860.269108581546\n",
            "17.548266062223856\n",
            "Epoch  4 Batch :  49  Training Loss : 20.45567360508100737571.23741760254\n",
            "27.22378106857765\n",
            "Epoch  4 Batch :  50  Training Loss : 20.5910357543509424956.766650390626\n",
            "17.92722438450398\n",
            "Epoch  4 Batch :  51  Training Loss : 20.53880415886374422942.11915740967\n",
            "16.795479136230828\n",
            "Epoch  4 Batch :  52  Training Loss : 20.46681713919772627410.2452545166\n",
            "18.982361135107116\n",
            "Epoch  4 Batch :  53  Training Loss : 20.4388085353469624306.8014465332\n",
            "17.024101012323943\n",
            "Epoch  4 Batch :  54  Training Loss : 20.37557321084653222066.42967224121\n",
            "15.760510955459768\n",
            "Epoch  4 Batch :  55  Training Loss : 20.2916629880213230799.325216674806\n",
            "21.11366163793104\n",
            "Epoch  4 Batch :  56  Training Loss : 20.30634153534113723727.474963378903\n",
            "16.81826164953933\n",
            "Epoch  4 Batch :  57  Training Loss : 20.245147151379725844.656111145017\n",
            "19.403150768365816\n",
            "Epoch  4 Batch :  58  Training Loss : 20.2306299723622225170.820826721192\n",
            "18.237410357661584\n",
            "Epoch  4 Batch :  59  Training Loss : 20.19684658906220825352.71331176758\n",
            "17.6627509996523\n",
            "Epoch  4 Batch :  60  Training Loss : 20.1546116625720439149.370095825194\n",
            "28.34941674187726\n",
            "Epoch  4 Batch :  61  Training Loss : 20.288952729445922146.42370300293\n",
            "15.896182172277937\n",
            "Epoch  4 Batch :  62  Training Loss : 20.2181015914270632489.371591186526\n",
            "23.811325571895424\n",
            "Epoch  4 Batch :  63  Training Loss : 20.27513689270433529711.769805908203\n",
            "20.16661072399728\n",
            "Epoch  4 Batch :  64  Training Loss : 20.2734411713182924255.54022674561\n",
            "16.94932108274648\n",
            "Epoch  4 Batch :  65  Training Loss : 20.2223008622633427878.88479003906\n",
            "20.36931963762811\n",
            "Epoch  4 Batch :  66  Training Loss : 20.22452841946583727582.526834106444\n",
            "19.088116747741488\n",
            "Epoch  4 Batch :  67  Training Loss : 20.20756705123114532212.06117553711\n",
            "22.2973251565762\n",
            "Epoch  4 Batch :  68  Training Loss : 20.23829878807445225089.43581848145\n",
            "17.605947931154383\n",
            "Epoch  4 Batch :  69  Training Loss : 20.2001487756553224442.404272460935\n",
            "17.67434745303468\n",
            "Epoch  4 Batch :  70  Training Loss : 20.16406589961788531640.002937316895\n",
            "21.849151167820068\n",
            "Epoch  4 Batch :  71  Training Loss : 20.18779949494467421602.370248413084\n",
            "16.267618091859596\n",
            "Epoch  4 Batch :  72  Training Loss : 20.13335253101293523209.506393432617\n",
            "16.792966498559075\n",
            "Epoch  4 Batch :  73  Training Loss : 20.08759381823959732813.60419921875\n",
            "22.296974914965986\n",
            "Epoch  4 Batch :  74  Training Loss : 20.1174503195467131203.809619140626\n",
            "21.934287219101126\n",
            "Epoch  4 Batch :  75  Training Loss : 20.1416748115407733838.10909423829\n",
            "23.616827345397052\n",
            "Epoch  4 Batch :  76  Training Loss : 20.1874005027757229251.02915344239\n",
            "20.402982261157604\n",
            "Epoch  4 Batch :  77  Training Loss : 20.1902002658715933229.824557495114\n",
            "22.725976428815876\n",
            "Epoch  4 Batch :  78  Training Loss : 20.22271021667856823791.99345703125\n",
            "17.387697030791788\n",
            "Epoch  4 Batch :  79  Training Loss : 20.1868239738192424530.494543457033\n",
            "17.467300612136267\n",
            "Epoch  4 Batch :  80  Training Loss : 20.15282993179820230732.409460449217\n",
            "21.710524825783974\n",
            "Epoch  4 Batch :  81  Training Loss : 20.1720607329585231592.004913330078\n",
            "23.26594807763061\n",
            "Epoch  4 Batch :  82  Training Loss : 20.2097910664301324821.809805297853\n",
            "17.621445035460994\n",
            "Epoch  4 Batch :  83  Training Loss : 20.17860617449074624278.618759155277\n",
            "16.693011110157858\n",
            "Epoch  4 Batch :  84  Training Loss : 20.1371109951534539563.810809326176\n",
            "27.30802753284924\n",
            "Epoch  4 Batch :  85  Training Loss : 20.2214747191263431412.643240356447\n",
            "22.245039473684212\n",
            "Epoch  4 Batch :  86  Training Loss : 20.24500454185376"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx_Nsi_si30G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
